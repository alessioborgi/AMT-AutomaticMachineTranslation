{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eb0051ba",
      "metadata": {
        "id": "eb0051ba"
      },
      "source": [
        "# AMT - AUTOMATIC MACHINE TRANSLATION\n",
        "\n",
        "**TO READ**\n",
        "Change in the third cell of \"0:..\" the \"hf_token\" and the \"gemini_api_key\" by putting yours.\n",
        "\n",
        "Notice that you should skip section 1.1. and that section 1.3 is skippable.\n",
        "\n",
        "***Author: @alessioborgi***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5815e93",
      "metadata": {
        "id": "e5815e93"
      },
      "source": [
        "### 0: IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dde1395d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dde1395d",
        "outputId": "fcc862f9-1e60-41b9-c992-439e8fe645c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (9.1.2)\n",
            "Requirement already satisfied: prometheus-eval[vllm] in /usr/local/lib/python3.11/dist-packages (0.1.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.172.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "\u001b[33mWARNING: prometheus-eval 0.1.20 does not provide the extra 'vllm'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: aiolimiter<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from prometheus-eval[vllm]) (1.2.1)\n",
            "Requirement already satisfied: fastchat<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from prometheus-eval[vllm]) (0.1.0)\n",
            "Requirement already satisfied: litellm<2.0.0,>=1.40.0 in /usr/local/lib/python3.11/dist-packages (from prometheus-eval[vllm]) (1.73.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (3.11.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (8.2.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (4.24.0)\n",
            "Requirement already satisfied: openai>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (1.86.0)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (1.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.6.15)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (1.20.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (0.25.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm<2.0.0,>=1.40.0->prometheus-eval[vllm]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets bitsandbytes accelerate\n",
        "!pip install huggingface-hub pandas transformers tiktoken protobuf sentencepiece tqdm google-generativeai tenacity prometheus-eval[vllm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dd111d96",
      "metadata": {
        "id": "dd111d96"
      },
      "outputs": [],
      "source": [
        "# Importing libraries for step 1).\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Importing libraries for step 2).\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
        "from transformers import MBartForConditionalGeneration, MBart50Tokenizer, AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig, pipeline, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3AWZ7m26DUsl",
      "metadata": {
        "id": "3AWZ7m26DUsl"
      },
      "outputs": [],
      "source": [
        "# PUT HERE AL THE KEYS (HF and GEMINI).\n",
        "hf_token = \"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\"\n",
        "gemini_api_key = \"AIzaSyD7EbS5LLkqPbeAMmr9BR29sP98iOLtpgQ\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "EedeMMm7XAUJ",
      "metadata": {
        "id": "EedeMMm7XAUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0777d681-c309-4baa-db36-b7ee60591d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f951ed",
      "metadata": {
        "id": "08f951ed"
      },
      "source": [
        "### 1: LOADING THE DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3c5cac4",
      "metadata": {
        "id": "b3c5cac4"
      },
      "source": [
        "#### 1.1: PUSH THE DATASET TO HUGGING-FACE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "317477c9",
      "metadata": {
        "id": "317477c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "a85f6793-504f-4f77-937b-3f9661001d41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef upload_to_hf_dataset(\\n    hf_token: str,\\n    data_file_path: str,\\n    repo_name: str,\\n    file_format: str = \"csv\",\\n    split_name: str = \"test\",\\n):\\n    \"\"\"\\n    Uploads a local file as a Hugging Face Dataset.\\n\\n    Args:\\n        hf_token: Your Hugging Face access token.\\n        data_file_path: Path to the local data file.\\n        repo_name: The target repo on HF (e.g. \"username/my-dataset\").\\n        file_format: One of \"csv\", \"json\", \"tsv\", etc. Default \"csv\".\\n        split_name: Name of the dataset split (e.g. \"train\", \"test\"). Default \"test\".\\n    \"\"\"\\n    # 1) Authenticate to HuggingFace.\\n    login(token=hf_token)\\n\\n    # 2) Load local file.\\n    data_files = { split_name: data_file_path }\\n    dataset = load_dataset(file_format, data_files=data_files)\\n\\n    # 3) Push to Hub.\\n    dataset.push_to_hub(repo_name, token=hf_token)\\n    print(f\"Dataset available at https://huggingface.co/datasets/{repo_name}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "'''\n",
        "def upload_to_hf_dataset(\n",
        "    hf_token: str,\n",
        "    data_file_path: str,\n",
        "    repo_name: str,\n",
        "    file_format: str = \"csv\",\n",
        "    split_name: str = \"test\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Uploads a local file as a Hugging Face Dataset.\n",
        "\n",
        "    Args:\n",
        "        hf_token: Your Hugging Face access token.\n",
        "        data_file_path: Path to the local data file.\n",
        "        repo_name: The target repo on HF (e.g. \"username/my-dataset\").\n",
        "        file_format: One of \"csv\", \"json\", \"tsv\", etc. Default \"csv\".\n",
        "        split_name: Name of the dataset split (e.g. \"train\", \"test\"). Default \"test\".\n",
        "    \"\"\"\n",
        "    # 1) Authenticate to HuggingFace.\n",
        "    login(token=hf_token)\n",
        "\n",
        "    # 2) Load local file.\n",
        "    data_files = { split_name: data_file_path }\n",
        "    dataset = load_dataset(file_format, data_files=data_files)\n",
        "\n",
        "    # 3) Push to Hub.\n",
        "    dataset.push_to_hub(repo_name, token=hf_token)\n",
        "    print(f\"Dataset available at https://huggingface.co/datasets/{repo_name}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d3534962",
      "metadata": {
        "id": "d3534962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "41a13d58-491e-40b6-a6df-394d7ab911fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nhf_token = \"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\"\\nlocal_path = \"/Users/alessioborgi/GitHub/AMT-AutomaticMachineTranslation/test_data/dataset_cleaned.csv\"\\nrepo_name  = \"Alessio-Borgi/archaic-italian-cleaned-test\"\\n\\nupload_to_hf_dataset(\\n    hf_token=hf_token,\\n    data_file_path=local_path,\\n    repo_name=repo_name,\\n    file_format=\"csv\",\\n    split_name=\"test\",\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "'''\n",
        "hf_token = \"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\"\n",
        "local_path = \"/Users/alessioborgi/GitHub/AMT-AutomaticMachineTranslation/test_data/dataset_cleaned.csv\"\n",
        "repo_name  = \"Alessio-Borgi/archaic-italian-cleaned-test\"\n",
        "\n",
        "upload_to_hf_dataset(\n",
        "    hf_token=hf_token,\n",
        "    data_file_path=local_path,\n",
        "    repo_name=repo_name,\n",
        "    file_format=\"csv\",\n",
        "    split_name=\"test\",\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab78d895",
      "metadata": {
        "id": "ab78d895"
      },
      "source": [
        "#### 1.2: LOADING DATASET FROM HUGGING-FACE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b45de4f9",
      "metadata": {
        "id": "b45de4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "aa355adc8ac148eeae1b026ab3cf8660",
            "8245971a57a2499790827e8b5c48dd72",
            "74993adca5834498ba9bfa99139ab70e",
            "b78ac95ee46743299eab05d9ad0739e4",
            "4459082f1f7b48ea9bcdef7617c3f292",
            "83f2f49a62df4224b62f987119f66268",
            "c55dd872b6cf4a518b4fcb2a63ecfb05",
            "5d4e06ac903c42ff8c19aa3ef3f40f85",
            "107de5ecf8264cfcaab5072f68613ba7",
            "f3130fc0a8c04d74819daf7d1f73c242",
            "1efca8fd049c41b8ae0ade9687682dc7",
            "4bdd1a40feb4450e878ae82af0d9ff42",
            "1fc5c3ad5eff455cbbfd75390f8b0351",
            "15b504d56e794ec689fb933572b5ee50",
            "4b18c578aeb444edae4ba3f1c2d537a1",
            "900254183f6e4b27a32280e6f27877c7",
            "8848c67c9b004216979e609ae6d5d22b",
            "85c84ffbd6d542268a38eb2676f28d99",
            "d8e9b0b5332b4526a155c0f65a493a50",
            "f16a3be7e8cd404ba07dd77a8e12fdea",
            "0c1379ebeb8c4b9ea9e88cffc48fdf64",
            "d0b39251d711483eb8c5defbbe1a8f53",
            "d3c52ec3cb8f4cd79166c4441f9c6c04",
            "f13639f7cc9644e8825e564587260913",
            "3564528245bf430eae0f0d5ba1b29f16",
            "315fb964ceac43bfb8204d5b10005d3f",
            "8bcd77cb09c74024bedaacccef6715dc",
            "c46ac2bc8294457b87accab57adf0206",
            "851b0f6af72c45fc9acabffb002ba902",
            "62b6265f9cc74f1db86aed537cc97e5f",
            "35aa1062e0964c6e9cb44bcc04d61df3",
            "feed346719bf4d65b7d40772778f138b",
            "735fae7cca2043e5a078cec25da97df8"
          ]
        },
        "outputId": "ecc4cad2-a879-4709-fbc6-37891fd0ec9a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/370 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa355adc8ac148eeae1b026ab3cf8660"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/11.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bdd1a40feb4450e878ae82af0d9ff42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/97 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3c52ec3cb8f4cd79166c4441f9c6c04"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "ds = load_dataset(\"Alessio-Borgi/archaic-italian-cleaned-test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8a715151",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a715151",
        "outputId": "2229c97e-4bf7-40c7-fc04-f98464ae966b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Author', 'Date', 'Region', 'Sentence'],\n",
              "    num_rows: 97\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "ds[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds.column_names)  # should include \"Sentence\", \"Author\", etc.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmeL7WAiQMIp",
        "outputId": "ede6c617-8ce1-4dde-ae58-ae11a947790b"
      },
      "id": "tmeL7WAiQMIp",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'test': ['Author', 'Date', 'Region', 'Sentence']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6632166c",
      "metadata": {
        "id": "6632166c"
      },
      "source": [
        "#### 1.3: EXPLORING THE TEST DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e34f5446",
      "metadata": {
        "id": "e34f5446"
      },
      "outputs": [],
      "source": [
        "def explore_dataset(dataset_name):\n",
        "    ''' Function to explore a dataset. '''\n",
        "\n",
        "    # Loading the dataset.\n",
        "    ds = load_dataset(dataset_name)\n",
        "    df = pd.DataFrame(ds[\"test\"])\n",
        "\n",
        "    # 1) Number of examples.\n",
        "    print(\"Number of examples:\", len(df))\n",
        "\n",
        "    # 2) Preview first 5 examples.\n",
        "    print(\"First 5 examples:\")\n",
        "    print(df.head(5), \"\\n\")\n",
        "\n",
        "    # 3) Sentence-length statistics.\n",
        "    df[\"length_tokens\"] = df[\"Sentence\"].apply(lambda x: len(x.split()))\n",
        "    print(\"Sentence length (tokens) stats:\")\n",
        "    print(df[\"length_tokens\"].describe(), \"\\n\")\n",
        "\n",
        "    # 4 Take out the column names.\n",
        "    print(\"Column names:\", df.columns.tolist(), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c325d352",
      "metadata": {
        "id": "c325d352"
      },
      "outputs": [],
      "source": [
        "# Explore the dataset.\n",
        "explore_dataset(dataset_name=\"Alessio-Borgi/archaic-italian-cleaned-test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e43c38e6",
      "metadata": {
        "id": "e43c38e6"
      },
      "source": [
        "### 2: AMT - TRANSFORMER-BASED"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd8ef6df",
      "metadata": {
        "id": "cd8ef6df"
      },
      "source": [
        "#### 2.1: mBART (MULTILINGUAL BART)\n",
        "\n",
        "**ARCHITECTURE & SIZE**\n",
        "This Transformer-based solution consists in 12-layer encoder + 12-layer decoder Transformer (≈610 M parameters).\n",
        "\n",
        "**DESCRIPTION**\n",
        "- **Pretraining**: It has been pretrained via Denoising auto-encoding on monolingual corpora in 50 languages (mBART-50).\n",
        "- **Multilingual MT**: It has been fine-tuned on many-to-many bitext and supports direct “it→it” by forcing Italian as both source & target.\n",
        "\n",
        "**REFERENCE INFORMATION**\n",
        "- Hugging-Face Reference page: https://huggingface.co/docs/transformers/model_doc/mbart\n",
        "- Paper: https://arxiv.org/abs/2001.08210\n",
        "- Specific Model employed: *facebook/mbart-large-50-many-to-many-mmt*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ec3f7d",
      "metadata": {
        "id": "c1ec3f7d"
      },
      "outputs": [],
      "source": [
        "# 1) Loading mBART-50 Model & Tokenizer.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "mBART_tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
        "mBART_model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "mBART_tokenizer.src_lang = \"it_IT\"\n",
        "mBART_tokenizer.model_max_length = 512\n",
        "\n",
        "\n",
        "# 2) Updated batched translation with device placement\n",
        "def modernize_mbart(sentences, batch_size=8):\n",
        "    \"\"\"\n",
        "    Translate sentences using mBART on GPU (if available),\n",
        "    showing a tqdm progress bar.\n",
        "    \"\"\"\n",
        "    translations = []\n",
        "    total_batches = (len(sentences) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in tqdm(\n",
        "        range(0, len(sentences), batch_size),\n",
        "        total=total_batches,\n",
        "        desc=\"mBART Translation\",\n",
        "        unit=\"batch\",\n",
        "        leave=True\n",
        "    ):\n",
        "        batch = sentences[i : i + batch_size]\n",
        "\n",
        "        # Tokenization.\n",
        "        inputs = mBART_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        inputs = { name: tensor.to(device) for name, tensor in inputs.items() }\n",
        "\n",
        "        # Generation of the Translations.\n",
        "        with torch.no_grad():\n",
        "            gen = mBART_model.generate(\n",
        "                **inputs,\n",
        "                forced_bos_token_id=mBART_tokenizer.lang_code_to_id[\"it_IT\"],\n",
        "                max_length=512,\n",
        "            )\n",
        "        # Decoding the extensions from tokenizer and add the translations to the list.\n",
        "        translations.extend(mBART_tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
        "    return translations\n",
        "\n",
        "# 4) Run translation.\n",
        "arch_sentences = ds[\"test\"][\"Sentence\"]\n",
        "mbart_translations = modernize_mbart(arch_sentences, batch_size=8)\n",
        "\n",
        "# 5) Write out a JSONL where each line has both fields.\n",
        "output_path = \"BorgiNonModernToModern-hw2_transl-mbart.jsonl\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for sent, trans in zip(arch_sentences, mbart_translations):\n",
        "        record = {\n",
        "            \"archaic_sentence\": sent,\n",
        "            \"mbart_translation\": trans\n",
        "        }\n",
        "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Saved {len(mbart_translations)} records to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-dpx6RkM-FlH",
      "metadata": {
        "id": "-dpx6RkM-FlH"
      },
      "outputs": [],
      "source": [
        "# 1) load your JSONL into a list of dicts\n",
        "with open(\"dataset_with_mbart_translations.jsonl\", encoding=\"utf-8\") as f:\n",
        "    records = [json.loads(line) for line in f]\n",
        "\n",
        "# 2) sample 10 of them\n",
        "sampled = random.sample(records, 10)\n",
        "\n",
        "# 3) print\n",
        "for rec in sampled:\n",
        "    print(\"Archaic Sentence: \", rec[\"archaic_sentence\"])\n",
        "    print(\"mBART Translation:\", rec[\"mbart_translation\"])\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2914d9a9",
      "metadata": {
        "id": "2914d9a9"
      },
      "source": [
        "#### 2.2: NLLB (No Language Left Behind)\n",
        "\n",
        "**ARCHITECTURE & SIZE**\n",
        "This Transformer-based solution comes from the Meta family. It's a many-to-many multilingual Seq2Seq that can be used as a rewriting model for Italian→Italian..\n",
        "\n",
        "**DESCRIPTION**\n",
        "- **High Capacity/Quality**: The flagship nllb-200-3.3B has shown state-of-the-art BLEU/COMET on many low-resource ↔ high-resource pairs, and handles morphological/orthographic variation robustly.\n",
        "- **Multilingual MT**: It supports 200 languages and has full support for ita_Latn (Italian in Latin script).\n",
        "\n",
        "**REFERENCE INFORMATION**\n",
        "- Hugging-Face Reference page: https://huggingface.co/docs/transformers/en/model_doc/nllb\n",
        "- Paper: https://arxiv.org/abs/2207.04672\n",
        "- Specific Model employed: *facebook/nllb-200-3.3B*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f5bbff7",
      "metadata": {
        "id": "6f5bbff7"
      },
      "outputs": [],
      "source": [
        "# Set up the 8-bit quantized NLLB pipeline for Italian→Italian.\n",
        "# 1) Set up the device specifics.\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"Using device:\", \"cuda\" if device == 0 else \"cpu\")\n",
        "\n",
        "# 2) 8-bit + offload config.\n",
        "bnb = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "# 3) Load model in 8-bit.\n",
        "model_name = \"facebook/nllb-200-3.3B\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 4) Load tokenizer with src/tgt languages set.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    src_lang=\"ita_Latn\",\n",
        "    tgt_lang=\"ita_Latn\"\n",
        ")\n",
        "\n",
        "# 5) Build the translation pipeline.\n",
        "translator = pipeline(\n",
        "    \"translation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    src_lang=\"ita_Latn\",\n",
        "    tgt_lang=\"ita_Latn\",\n",
        ")\n",
        "\n",
        "# 6) Taking the sentences to translate and translate in batches.\n",
        "arch = ds[\"test\"][\"Sentence\"]\n",
        "results = translator(arch, batch_size=8)\n",
        "\n",
        "# 7) Extract the Italian text.\n",
        "italian_translations = [r[\"translation_text\"] for r in results]\n",
        "\n",
        "\n",
        "# 8) Attach & save to jsonl file.\n",
        "output_path = \"BorgiNonModernToModern-hw2_transl-nllb.jsonl\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for i, rec in enumerate(ds[\"test\"]):\n",
        "        fout.write(\n",
        "            json.dumps({\n",
        "                \"archaic_sentence\": rec[\"Sentence\"],\n",
        "                \"nllb_translation\": italian_translations[i]\n",
        "            }, ensure_ascii=False)\n",
        "            + \"\\n\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KCR1uTEGqx5P",
      "metadata": {
        "id": "KCR1uTEGqx5P"
      },
      "outputs": [],
      "source": [
        "# 1) Sample 10 random indices\n",
        "input_path = \"BorgiNonModernToModern-hw2_transl-nllb.jsonl\"\n",
        "records = []\n",
        "with open(input_path, \"r\", encoding=\"utf-8\") as fin:\n",
        "    for line in fin:\n",
        "        records.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(records)} records.\")\n",
        "\n",
        "# If there are fewer than 10, sample all of them\n",
        "n_samples = min(10, len(records))\n",
        "\n",
        "indices = random.sample(range(len(records)), n_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    print(f\"Archaic Sentence: {records[idx]['archaic_sentence']}\")\n",
        "    print(f\"NLLB Translation: {records[idx]['nllb_translation']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UQ68lRmDuKzX",
      "metadata": {
        "id": "UQ68lRmDuKzX"
      },
      "source": [
        "### 3: AMT - LLM-BASED"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EyWlIlVwuQfQ",
      "metadata": {
        "id": "EyWlIlVwuQfQ"
      },
      "source": [
        "#### 3.1: LLAMA-2-7b-chat-hf\n",
        "\n",
        "**Hugging-Face Reference Page:** https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "**#Params:** 7B\n",
        "\n",
        "**GPU-RAM:** 12.9GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbNSGbJ1eNiY",
      "metadata": {
        "id": "dbNSGbJ1eNiY"
      },
      "outputs": [],
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"Using device:\", \"cuda\" if device == 0 else \"cpu\")\n",
        "# 1) Load LLAMA 3.1-8B model & tokenizer.\n",
        "llama_checkpoint = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(llama_checkpoint, device_map=\"auto\", torch_dtype=\"auto\", hf_token=hf_token)\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llama_checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "# 2) Taking the sentences to translate and translate in batches.\n",
        "sentences = ds[\"test\"][\"Sentence\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HSf0mLwPdM3I",
      "metadata": {
        "id": "HSf0mLwPdM3I"
      },
      "source": [
        "##### 3.1.1: ZERO-SHOT TRANSLATION\n",
        "\n",
        "\n",
        "**TOTAL TIME-TO-RUN:** 276.91 seconds\n",
        "\n",
        "**AVG-per-SENTENCE TIME:** 2.85 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x5w6AMMmdpx3",
      "metadata": {
        "id": "x5w6AMMmdpx3"
      },
      "outputs": [],
      "source": [
        "# 3) Set pad token for batching.\n",
        "if llama_tokenizer.pad_token is None:\n",
        "    llama_tokenizer.padding_side = \"left\"\n",
        "\n",
        "# 4) Build translation pipeline.\n",
        "llama_translator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llama_model,\n",
        "    tokenizer=llama_tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# 5) Starting the batched translation.\n",
        "batch_size = 8\n",
        "n = len(sentences)\n",
        "llama_outputs = []\n",
        "\n",
        "total_start = time.time()\n",
        "for start in tqdm(range(0, n, batch_size), desc=\"Translating with Llama\"):\n",
        "    # Format prompts in each batch\n",
        "    batch_sentences = sentences[start:start+batch_size]\n",
        "    batch_prompts = [f\"Traduci la seguente frase dall'italiano arcaico all'italiano moderno. Solo la traduzione, senza spiegazioni:\\n{s}\\nRisposta:\"\n",
        "    for s in batch_sentences\n",
        "]\n",
        "\n",
        "    batch_results = llama_translator(batch_prompts)\n",
        "    for i, r in enumerate(batch_results):\n",
        "        # Remove prompt prefix from output.\n",
        "        completion = r[0][\"generated_text\"]\n",
        "        result = completion.replace(batch_prompts[i], \"\").strip()\n",
        "        llama_outputs.append(result)\n",
        "total_end = time.time()\n",
        "print(f\"\\nTotal time: {total_end - total_start:.2f} seconds\")\n",
        "print(f\"Average per sentence: {(total_end - total_start)/n:.2f} seconds\")\n",
        "\n",
        "# 6) Save translations.\n",
        "output_path = \"BorgiNonModernToModern-hw2_transl-llama.jsonl\"\n",
        "sentences_out = ds[\"test\"][\"Sentence\"]\n",
        "translations_out = llama_outputs\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for s, t in zip(sentences_out, translations_out):\n",
        "        entry = {\n",
        "            \"archaic_sentence\": s,\n",
        "            \"llama_translation\": t\n",
        "        }\n",
        "        json.dump(entry, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RzMrwS-WTXwO",
      "metadata": {
        "id": "RzMrwS-WTXwO"
      },
      "outputs": [],
      "source": [
        "llama_outputs = \"BorgiNonModernToModern-hw2_transl-llama.jsonl\"\n",
        "with open(llama_outputs, encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "n_samples = min(10, len(data))\n",
        "indices = random.sample(range(len(data)), n_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    print(f\"Archaic Sentence: {data[idx]['archaic_sentence']}\")\n",
        "    print(f\"Llama Translation: {data[idx]['llama_translation']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f_tdaEVZHuJ1",
      "metadata": {
        "id": "f_tdaEVZHuJ1"
      },
      "source": [
        "##### 3.1.2: FEW-SHOT TRANSLATION\n",
        "\n",
        "**TOTAL TIME-TO-RUN:** 925.33 seconds\n",
        "\n",
        "**AVG-per-SENTENCE TIME:** 9.54 seconds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_examples = [\n",
        "    (\"Ove non sia chi ti conforti, confortati da te stesso.\", \"Se non c'è nessuno a consolarti, consola te stesso.\"),\n",
        "    (\"Così nel suo cammino solingo andava pensoso e lento.\", \"Così nel suo cammino solitario procedeva pensieroso e lento.\"),\n",
        "    (\"Non è oro tutto quel che riluce.\", \"Non è tutto oro ciò che luccica.\"),\n",
        "]\n",
        "\n",
        "def build_fewshot_prompt(archaic_sentence):\n",
        "    intro = \"Traduci dall'italiano arcaico all'italiano moderno. Solo la traduzione, senza spiegazioni e senza riportare gli esempi precedenti!\"\n",
        "    shots = \"\\n\".join([\n",
        "        f\"Esempio {i+1}:\\nFrase: {a}\\nTraduzione: {m}\" for i, (a, m) in enumerate(few_shot_examples)\n",
        "    ])\n",
        "    test = f\"Frase: {archaic_sentence}\\nTraduzione:\"\n",
        "    return f\"{intro}\\n\\n{shots}\\n\\n{test}\"\n",
        "\n",
        "def clean_translation(raw, prompt):\n",
        "    # Remove the prompt\n",
        "    answer = raw.replace(prompt, \"\").strip()\n",
        "    # Cut at first \"Esempio\" or \"Frase:\" if the model keeps generating\n",
        "    for stop_word in [\"Esempio\", \"Frase:\", \"Traduzione:\", \"\\n\\n\"]:\n",
        "        idx = answer.find(stop_word)\n",
        "        if idx > 1:\n",
        "            answer = answer[:idx].strip()\n",
        "    # Optionally: cut at first linebreak if output is multi-line\n",
        "    answer = answer.split(\"\\n\")[0].strip()\n",
        "    return answer\n",
        "\n",
        "\n",
        "\n",
        "# Tokenization step.\n",
        "if llama_tokenizer.pad_token is None:\n",
        "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Building the translation pipeline.\n",
        "llama_translator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llama_model,\n",
        "    tokenizer=llama_tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "batch_size = 8\n",
        "n = len(sentences)\n",
        "llama_outputs = []\n",
        "\n",
        "total_start = time.time()\n",
        "for start in tqdm(range(0, n, batch_size), desc=\"Few-Shot Translating with Llama\"):\n",
        "    batch_sentences = sentences[start:start+batch_size]\n",
        "    batch_prompts = [build_fewshot_prompt(s) for s in batch_sentences]\n",
        "    batch_results = llama_translator(batch_prompts)\n",
        "    for i, r in enumerate(batch_results):\n",
        "        completion = r[0][\"generated_text\"]\n",
        "        result = clean_translation(completion, batch_prompts[i])\n",
        "        llama_outputs.append(result)\n",
        "\n",
        "total_end = time.time()\n",
        "print(f\"\\nTotal time: {total_end - total_start:.2f} seconds\")\n",
        "print(f\"Average per sentence: {(total_end - total_start)/n:.2f} seconds\")\n",
        "\n",
        "# Saving the Translations.\n",
        "output_path = \"BorgiNonModernToModern-hw2_transl-llama-fewshot.jsonl\"\n",
        "sentences_out = ds[\"test\"][\"Sentence\"]\n",
        "translations_out = llama_outputs\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for s, t in zip(sentences_out, translations_out):\n",
        "        entry = {\n",
        "            \"archaic_sentence\": s,\n",
        "            \"llama_translation\": t\n",
        "        }\n",
        "        json.dump(entry, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "lL1U-I81f2nr"
      },
      "id": "lL1U-I81f2nr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pritning out 10 translations to see how the translation process has gone.\n",
        "with open(output_path, encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "n_samples = min(10, len(data))\n",
        "indices = random.sample(range(len(data)), n_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    print(f\"Archaic Sentence: {data[idx]['archaic_sentence']}\")\n",
        "    print(f\"Llama Translation: {data[idx]['llama_translation']}\\n\")"
      ],
      "metadata": {
        "id": "8bHYbzPViHVW"
      },
      "id": "8bHYbzPViHVW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "M-878pRaHwUH",
      "metadata": {
        "id": "M-878pRaHwUH"
      },
      "source": [
        "##### 3.1.3: CHAIN-OF-THOUGHT TRANSLATION\n",
        "\n",
        "**TOTAL TIME-TO-RUN:** x seconds\n",
        "\n",
        "**AVG-per-SENTENCE TIME:** x seconds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1. Define your few-shot CoT examples\n",
        "few_shot_cot_examples = [\n",
        "    (\n",
        "        \"Ove non sia chi ti conforti, confortati da te stesso.\",\n",
        "        \"Qui la frase suggerisce che, in assenza di consolatori esterni, una persona deve trovare forza dentro di sé.\",\n",
        "        \"Se non c'è nessuno a consolarti, consola te stesso.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Così nel suo cammino solingo andava pensoso e lento.\",\n",
        "        \"La parola 'solingo' oggi si direbbe 'solitario', e 'pensoso e lento' suggerisce un'andatura riflessiva.\",\n",
        "        \"Così nel suo cammino solitario procedeva pensieroso e lento.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Non è oro tutto quel che riluce.\",\n",
        "        \"Questa è una metafora che indica che non tutto ciò che sembra prezioso lo è veramente; si usa una versione moderna.\",\n",
        "        \"Non è tutto oro ciò che luccica.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# 2. Chain-of-Thought few-shot prompt builder\n",
        "def build_cot_fewshot_prompt(archaic_sentence):\n",
        "    intro = (\n",
        "        \"Traduci dall'italiano arcaico all'italiano moderno. \"\n",
        "        \"Prima spiega brevemente il ragionamento, poi fornisci la traduzione finale. \"\n",
        "        \"Segui l'esempio. Non ripetere gli esempi precedenti!\"\n",
        "    )\n",
        "    shots = \"\\n\".join([\n",
        "        f\"Esempio {i+1}:\\nFrase: {a}\\nRagionamento: {r}\\nTraduzione: {m}\"\n",
        "        for i, (a, r, m) in enumerate(few_shot_cot_examples)\n",
        "    ])\n",
        "    test = f\"Frase: {archaic_sentence}\\nRagionamento:\"\n",
        "    return f\"{intro}\\n\\n{shots}\\n\\n{test}\"\n",
        "\n",
        "# 3. Clean translation (extract only model translation)\n",
        "def clean_translation_cot(raw, prompt):\n",
        "    answer = raw.replace(prompt, \"\").strip()\n",
        "    # Extract after 'Traduzione:'\n",
        "    if \"Traduzione:\" in answer:\n",
        "        answer = answer.split(\"Traduzione:\", 1)[-1].strip()\n",
        "    # Cut if model keeps generating\n",
        "    for stop_word in [\"Esempio\", \"Frase:\", \"\\n\\n\"]:\n",
        "        idx = answer.find(stop_word)\n",
        "        if idx > 1:\n",
        "            answer = answer[:idx].strip()\n",
        "    answer = answer.split(\"\\n\")[0].strip()\n",
        "    return answer\n",
        "\n",
        "# 4. Load your Llama model & tokenizer (change checkpoint if you want)\n",
        "llama_checkpoint = \"meta-llama/Llama-2-7b-chat-hf\"  # Or Llama-3 if you have it!\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(llama_checkpoint, trust_remote_code=True)\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llama_checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 5. Prepare your test set\n",
        "# Example: If using HuggingFace datasets: ds[\"test\"][\"Sentence\"]\n",
        "# Here, I'll show as loading from your test file (change as needed)\n",
        "with open(\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/BorgiNonModernToModern-hw2_transl-llama.jsonl\", encoding=\"utf-8\") as f:\n",
        "    sentences = [json.loads(line)[\"archaic_sentence\"] for line in f]\n",
        "\n",
        "# 6. Tokenizer padding\n",
        "if llama_tokenizer.pad_token is None:\n",
        "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"left\"\n",
        "\n",
        "# 7. Build pipeline\n",
        "llama_translator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llama_model,\n",
        "    tokenizer=llama_tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# 8. Translation (CoT Few-shot) batched\n",
        "batch_size = 8\n",
        "n = len(sentences)\n",
        "llama_outputs = []\n",
        "\n",
        "total_start = time.time()\n",
        "for start in tqdm(range(0, n, batch_size), desc=\"CoT Few-Shot Translating with Llama\"):\n",
        "    batch_sentences = sentences[start:start+batch_size]\n",
        "    batch_prompts = [build_cot_fewshot_prompt(s) for s in batch_sentences]\n",
        "    batch_results = llama_translator(batch_prompts)\n",
        "    for i, r in enumerate(batch_results):\n",
        "        completion = r[0][\"generated_text\"]\n",
        "        result = clean_translation_cot(completion, batch_prompts[i])\n",
        "        llama_outputs.append(result)\n",
        "total_end = time.time()\n",
        "print(f\"\\nTotal time: {total_end - total_start:.2f} seconds\")\n",
        "print(f\"Average per sentence: {(total_end - total_start)/n:.2f} seconds\")\n",
        "\n",
        "# 9. Save as requested\n",
        "output_path = \"BorgiNonModernToModern-hw2_transl-llama-fewshot-cot.jsonl\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for s, t in zip(sentences, llama_outputs):\n",
        "        entry = {\n",
        "            \"archaic_sentence\": s,\n",
        "            \"llama_translation\": t\n",
        "        }\n",
        "        json.dump(entry, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "dUPzCiHnbFWw"
      },
      "id": "dUPzCiHnbFWw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Print out 10 random translations\n",
        "with open(output_path, encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "n_samples = min(10, len(data))\n",
        "indices = random.sample(range(len(data)), n_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    print(f\"Archaic Sentence: {data[idx]['archaic_sentence']}\")\n",
        "    print(f\"Llama Translation: {data[idx]['llama_translation']}\\n\")"
      ],
      "metadata": {
        "id": "oHyeyZaKcI6r"
      },
      "id": "oHyeyZaKcI6r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3.2.4: RE-ACT (REASON+ACT) TRANSLATION\n",
        "\n",
        "**TOTAL TIME-TO-RUN:** 1030.91 seconds\n",
        "\n",
        "**AVG-per-SENTENCE TIME:** 10.63 seconds"
      ],
      "metadata": {
        "id": "-CDyR6plvzGm"
      },
      "id": "-CDyR6plvzGm"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# --- SETUP ---\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"Using device:\", \"cuda\" if device == 0 else \"cpu\")\n",
        "\n",
        "hf_token = \"<YOUR_HF_TOKEN_HERE>\"  # <-- Insert your HF token if needed!\n",
        "\n",
        "# 1) Load LLAMA 3.1-8B model & tokenizer.\n",
        "llama_checkpoint = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(llama_checkpoint, device_map=\"auto\", torch_dtype=\"auto\", hf_token=hf_token)\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llama_checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "# 2) Load sentences to translate.\n",
        "# If using Huggingface datasets:\n",
        "# sentences = ds[\"test\"][\"Sentence\"]\n",
        "# If using a file (adapt path as needed):\n",
        "with open(\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/BorgiNonModernToModern-hw2_transl-llama.jsonl\", encoding=\"utf-8\") as f:\n",
        "    sentences = [json.loads(line)[\"archaic_sentence\"] for line in f]\n",
        "\n",
        "# 3) Robust ReAct few-shot examples for Llama.\n",
        "few_shot_react_examples = [\n",
        "    (\n",
        "        \"Ove non sia chi ti conforti, confortati da te stesso.\",\n",
        "        \"La frase sottolinea il valore dell'autosufficienza emotiva. 'Conforti' è un verbo arcaico per 'consolare'. Il messaggio è che bisogna consolarsi da soli se nessuno lo fa per noi.\",\n",
        "        \"Traduci in italiano moderno.\",\n",
        "        \"Se non c'è nessuno a consolarti, consola te stesso.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Così nel suo cammino solingo andava pensoso e lento.\",\n",
        "        \"'Solingo' oggi si direbbe 'solitario'. Descrive una persona che cammina da sola, lentamente, persa nei pensieri.\",\n",
        "        \"Traduci in italiano moderno.\",\n",
        "        \"Così nel suo cammino solitario procedeva pensieroso e lento.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Non è oro tutto quel che riluce.\",\n",
        "        \"È un proverbio che consiglia di non fidarsi delle apparenze; la parola moderna è 'luccica' invece di 'riluce'.\",\n",
        "        \"Traduci in italiano moderno.\",\n",
        "        \"Non è tutto oro ciò che luccica.\"\n",
        "    ),\n",
        "    # (Add more real examples for better performance!)\n",
        "]\n",
        "\n",
        "# 4) Robust ReAct prompt builder.\n",
        "def build_react_prompt(archaic_sentence):\n",
        "    intro = (\n",
        "        \"Traduci la seguente frase dall'italiano arcaico all'italiano moderno seguendo il metodo Reason+Act (ReAct). \"\n",
        "        \"Per ciascun esempio, prima ragiona brevemente sul significato e sulle parole arcaiche (Pensiero), poi indica l'azione (Azione) e infine fornisci la traduzione moderna (Traduzione). \"\n",
        "        \"Non riportare gli esempi precedenti. \"\n",
        "        \"Ecco degli esempi:\"\n",
        "    )\n",
        "    shots = \"\\n\".join([\n",
        "        f\"Esempio {i+1}:\\nFrase: {a}\\nPensiero: {thought}\\nAzione: {action}\\nTraduzione: {modern}\"\n",
        "        for i, (a, thought, action, modern) in enumerate(few_shot_react_examples)\n",
        "    ])\n",
        "    test = f\"Esempio {len(few_shot_react_examples)+1}:\\nFrase: {archaic_sentence}\\nPensiero:\"\n",
        "    return f\"{intro}\\n\\n{shots}\\n\\n{test}\"\n",
        "\n",
        "# 5) Improved clean_translation function\n",
        "def clean_translation_react(raw, prompt):\n",
        "    answer = raw.replace(prompt, \"\").strip()\n",
        "    if \"Traduzione:\" in answer:\n",
        "        answer = answer.split(\"Traduzione:\", 1)[-1].strip()\n",
        "    for stop_word in [\"Esempio\", \"Frase:\", \"Pensiero:\", \"Azione:\", \"\\n\\n\"]:\n",
        "        idx = answer.find(stop_word)\n",
        "        if idx > 1:\n",
        "            answer = answer[:idx].strip()\n",
        "    answer = answer.split(\"\\n\")[0].strip()\n",
        "    return answer\n",
        "\n",
        "# 6) Tokenizer padding setup\n",
        "if llama_tokenizer.pad_token is None:\n",
        "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"left\"\n",
        "\n",
        "# 7) Build translation pipeline\n",
        "llama_translator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llama_model,\n",
        "    tokenizer=llama_tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# 8) ReAct translation in batch\n",
        "batch_size = 8\n",
        "n = len(sentences)\n",
        "llama_outputs = []\n",
        "\n",
        "total_start = time.time()\n",
        "for start in tqdm(range(0, n, batch_size), desc=\"ReAct Translating with Llama\"):\n",
        "    batch_sentences = sentences[start:start+batch_size]\n",
        "    batch_prompts = [build_react_prompt(s) for s in batch_sentences]\n",
        "    batch_results = llama_translator(batch_prompts)\n",
        "    for i, r in enumerate(batch_results):\n",
        "        completion = r[0][\"generated_text\"]\n",
        "        result = clean_translation_react(completion, batch_prompts[i])\n",
        "        llama_outputs.append(result)\n",
        "total_end = time.time()\n",
        "print(f\"\\nTotal time: {total_end - total_start:.2f} seconds\")\n",
        "print(f\"Average per sentence: {(total_end - total_start)/n:.2f} seconds\")\n",
        "\n",
        "# 9) Save results\n",
        "output_path = \"BorgiNonModernToModern-hw2_transl-llama-react.jsonl\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for archaic, modern in zip(sentences, llama_outputs):\n",
        "        record = {\n",
        "            \"archaic_sentence\": archaic,\n",
        "            \"llama_translation\": modern\n",
        "        }\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
      ],
      "metadata": {
        "id": "-sljbsDGvzGm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-sljbsDGvzGm"
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Print 10 random translations\n",
        "with open(output_path, encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "n_samples = min(10, len(data))\n",
        "indices = random.sample(range(len(data)), n_samples)\n",
        "for idx in indices:\n",
        "    print(f\"Archaic Sentence: {data[idx]['archaic_sentence']}\")\n",
        "    print(f\"Llama Translation: {data[idx]['llama_translation']}\\n\")"
      ],
      "metadata": {
        "id": "vI32o-F3vzGn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vI32o-F3vzGn"
    },
    {
      "cell_type": "markdown",
      "id": "3mnfnbnICE_e",
      "metadata": {
        "id": "3mnfnbnICE_e"
      },
      "source": [
        "#### 3.2: GEMMA 2B-Instruct\n",
        "\n",
        "**Hugging-Face Reference Page:** https://huggingface.co/google/gemma-2b-it\n",
        "\n",
        "**#Params:** 2B\n",
        "\n",
        "**GPU-RAM:** 5.8GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y058wc-7CWVR",
      "metadata": {
        "id": "Y058wc-7CWVR"
      },
      "outputs": [],
      "source": [
        "# 1) Load model & tokenizer.\n",
        "gemma_checkpoint = \"google/gemma-2b-it\"\n",
        "gemma_tokenizer = AutoTokenizer.from_pretrained(gemma_checkpoint, trust_remote_code=True)\n",
        "gemma_model     = AutoModelForCausalLM.from_pretrained(\n",
        "    gemma_checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "# 2) Taking the sentences to translate and translate in batches.\n",
        "sentences = ds[\"test\"][\"Sentence\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y7ymUYc6CafN",
      "metadata": {
        "id": "y7ymUYc6CafN"
      },
      "source": [
        "##### 3.2.1: ZERO-SHOT TRANSLATION\n",
        "\n",
        "**TOTAL TIME-TO-RUN:** 105.27 seconds\n",
        "\n",
        "**AVG-per-SENTENCE TIME:** 1.09 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5llYCPVFlCKI",
      "metadata": {
        "id": "5llYCPVFlCKI"
      },
      "outputs": [],
      "source": [
        "# 3) Set pad token for batching.\n",
        "if gemma_tokenizer.pad_token is None:\n",
        "    gemma_tokenizer.pad_token = gemma_tokenizer.eos_token\n",
        "gemma_tokenizer.padding_side = \"left\"\n",
        "\n",
        "# 4) Use \"text-generation\" pipeline.\n",
        "falcon_translator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=gemma_model,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_new_tokens=256,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# 5) Prepare translation prompts.\n",
        "prompts = [f\"Traduci la seguente frase dall'italiano arcaico all'italiano moderno. Solo la traduzione, senza spiegazioni:\\n{s}\\nRisposta:\" for s in sentences]\n",
        "\n",
        "# 5) Batched generation.\n",
        "batch_size = 8\n",
        "n = len(sentences)\n",
        "total_start = time.time()\n",
        "gemma_outputs = []\n",
        "for start in tqdm(range(0, len(prompts), batch_size), desc=\"Translating with Gemma\"):\n",
        "    batch_prompts = prompts[start:start+batch_size]\n",
        "    batch_results = falcon_translator(batch_prompts)\n",
        "    for i, r in enumerate(batch_results):\n",
        "        # Remove prompt from output\n",
        "        gen = r[0][\"generated_text\"]\n",
        "        translation = gen.replace(batch_prompts[i], \"\").strip()\n",
        "        gemma_outputs.append(translation)\n",
        "\n",
        "# 5.5) Compute time complexity.\n",
        "total_end = time.time()\n",
        "print(f\"\\nTotal time: {total_end - total_start:.2f} seconds\")\n",
        "print(f\"Average per sentence: {(total_end - total_start)/n:.2f} seconds\")\n",
        "\n",
        "# 6) Save or attach as usual.\n",
        "jsonl_path = \"BorgiNonModernToModern-hw2_transl-gemma.jsonl\"\n",
        "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for archaic, modern in zip(sentences, gemma_outputs):\n",
        "        record = {\n",
        "            \"archaic_sentence\": archaic,\n",
        "            \"gemma_translation\": modern\n",
        "        }\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_r3Lu1vFGcZ7",
      "metadata": {
        "id": "_r3Lu1vFGcZ7"
      },
      "outputs": [],
      "source": [
        "gemma_outputs = \"BorgiNonModernToModern-hw2_transl-gemma.jsonl\"\n",
        "with open(gemma_outputs, encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "n_samples = min(10, len(data))\n",
        "indices = random.sample(range(len(data)), n_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    print(f\"Archaic Sentence: {data[idx]['archaic_sentence']}\")\n",
        "    print(f\"Gemma Translation: {data[idx]['gemma_translation']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pxtTAd4nHo-w",
      "metadata": {
        "id": "pxtTAd4nHo-w"
      },
      "source": [
        "##### 3.2.2: FEW-SHOT TRANSLATION\n",
        "\n",
        "**TOTAL TIME-TO-RUN:** 2520.21 seconds\n",
        "\n",
        "**AVG-per-SENTENCE TIME:** 25.98 seconds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_examples = [\n",
        "    (\"Ove non sia chi ti conforti, confortati da te stesso.\", \"Se non c'è nessuno a consolarti, consola te stesso.\"),\n",
        "    (\"Così nel suo cammino solingo andava pensoso e lento.\", \"Così nel suo cammino solitario procedeva pensieroso e lento.\"),\n",
        "    (\"Non è oro tutto quel che riluce.\", \"Non è tutto oro ciò che luccica.\"),\n",
        "]\n",
        "\n",
        "def build_fewshot_prompt(archaic_sentence):\n",
        "    intro = \"Traduci dall'italiano arcaico all'italiano moderno. Solo la traduzione, senza spiegazioni.\"\n",
        "    shots = \"\\n\".join([\n",
        "        f\"Esempio {i+1}:\\nFrase: {a}\\nTraduzione: {m}\" for i, (a, m) in enumerate(few_shot_examples)\n",
        "    ])\n",
        "    test = f\"Frase: {archaic_sentence}\\nTraduzione:\"\n",
        "    return f\"{intro}\\n\\n{shots}\\n\\n{test}\"\n",
        "\n",
        "def clean_translation(raw, prompt):\n",
        "    # Remove the prompt\n",
        "    answer = raw.replace(prompt, \"\").strip()\n",
        "    # Cut at first \"Esempio\" or \"Frase:\" if the model keeps generating\n",
        "    for stop_word in [\"Esempio\", \"Frase:\", \"Traduzione:\", \"\\n\\n\"]:\n",
        "        idx = answer.find(stop_word)\n",
        "        if idx > 1:\n",
        "            answer = answer[:idx].strip()\n",
        "    # Optionally: cut at first linebreak if output is multi-line\n",
        "    answer = answer.split(\"\\n\")[0].strip()\n",
        "    return answer\n",
        "\n",
        "# Tokenization step.\n",
        "if gemma_tokenizer.pad_token is None:\n",
        "    gemma_tokenizer.pad_token = gemma_tokenizer.eos_token\n",
        "gemma_tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Building Translation Pipeline.\n",
        "gemma_translator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=gemma_model,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# Batched few-shot Translations.\n",
        "batch_size = 8\n",
        "n = len(sentences)\n",
        "gemma_outputs = []\n",
        "\n",
        "total_start = time.time()\n",
        "for start in tqdm(range(0, n, batch_size), desc=\"Few-Shot Translating with Gemma\"):\n",
        "    batch_sentences = sentences[start:start+batch_size]\n",
        "    batch_prompts = [build_fewshot_prompt(s) for s in batch_sentences]\n",
        "    batch_results = gemma_translator(batch_prompts)\n",
        "    for i, r in enumerate(batch_results):\n",
        "        completion = r[0][\"generated_text\"]\n",
        "        result = clean_translation(completion, batch_prompts[i])\n",
        "        gemma_outputs.append(result)\n",
        "total_end = time.time()\n",
        "print(f\"\\nTotal time: {total_end - total_start:.2f} seconds\")\n",
        "print(f\"Average per sentence: {(total_end - total_start)/n:.2f} seconds\")\n",
        "\n",
        "# Saving the Translations.\n",
        "jsonl_path = \"BorgiNonModernToModern-hw2_transl-gemma-fewshot.jsonl\"\n",
        "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for archaic, modern in zip(sentences, gemma_outputs):\n",
        "        record = {\n",
        "            \"archaic_sentence\": archaic,\n",
        "            \"gemma_translation\": modern\n",
        "        }\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
      ],
      "metadata": {
        "id": "H7mwXHzCjj5A"
      },
      "id": "H7mwXHzCjj5A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pritning out 10 translations to see how the translation process has gone.\n",
        "with open(jsonl_path, encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "n_samples = min(10, len(data))\n",
        "indices = random.sample(range(len(data)), n_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    print(f\"Archaic Sentence: {data[idx]['archaic_sentence']}\")\n",
        "    print(f\"Gemma Translation: {data[idx]['gemma_translation']}\\n\")"
      ],
      "metadata": {
        "id": "lYH02Efmjjsw"
      },
      "id": "lYH02Efmjjsw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "LtQD0iGvICtQ",
      "metadata": {
        "id": "LtQD0iGvICtQ"
      },
      "source": [
        "##### 3.2.3: CHAIN-OF-THOUGHT TRANSLATION\n",
        "\n",
        "**TOTAL TIME-TO-RUN:** 251.41 seconds\n",
        "\n",
        "**AVG-per-SENTENCE TIME:** 2.59 seconds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1. Define Chain-of-Thought few-shot examples for GEMMA\n",
        "few_shot_cot_examples = [\n",
        "    (\n",
        "        \"Ove non sia chi ti conforti, confortati da te stesso.\",\n",
        "        \"Qui il significato è che bisogna trovare la forza dentro di sé quando non si ha nessun altro a cui appoggiarsi.\",\n",
        "        \"Se non c'è nessuno a consolarti, consola te stesso.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Così nel suo cammino solingo andava pensoso e lento.\",\n",
        "        \"La parola 'solingo' corrisponde a 'solitario', e l'espressione suggerisce una camminata lenta e riflessiva.\",\n",
        "        \"Così nel suo cammino solitario procedeva pensieroso e lento.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Non è oro tutto quel che riluce.\",\n",
        "        \"Questo è un modo di dire che insegna a non fidarsi delle apparenze; la versione moderna usa 'luccica'.\",\n",
        "        \"Non è tutto oro ciò che luccica.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "# 2. Chain-of-Thought prompt builder\n",
        "def build_cot_fewshot_prompt(archaic_sentence):\n",
        "    intro = (\n",
        "        \"Traduci dall'italiano arcaico all'italiano moderno. \"\n",
        "        \"Prima spiega brevemente il ragionamento, poi fornisci la traduzione finale. \"\n",
        "        \"Segui l'esempio. Non ripetere gli esempi precedenti!\"\n",
        "    )\n",
        "    shots = \"\\n\".join([\n",
        "        f\"Esempio {i+1}:\\nFrase: {a}\\nRagionamento: {r}\\nTraduzione: {m}\"\n",
        "        for i, (a, r, m) in enumerate(few_shot_cot_examples)\n",
        "    ])\n",
        "    test = f\"Frase: {archaic_sentence}\\nRagionamento:\"\n",
        "    return f\"{intro}\\n\\n{shots}\\n\\n{test}\"\n",
        "\n",
        "# 3. Clean translation (extract only model translation)\n",
        "def clean_translation_cot(raw, prompt):\n",
        "    answer = raw.replace(prompt, \"\").strip()\n",
        "    # Extract after 'Traduzione:'\n",
        "    if \"Traduzione:\" in answer:\n",
        "        answer = answer.split(\"Traduzione:\", 1)[-1].strip()\n",
        "    # Cut if model keeps generating\n",
        "    for stop_word in [\"Esempio\", \"Frase:\", \"\\n\\n\"]:\n",
        "        idx = answer.find(stop_word)\n",
        "        if idx > 1:\n",
        "            answer = answer[:idx].strip()\n",
        "    answer = answer.split(\"\\n\")[0].strip()\n",
        "    return answer\n",
        "\n",
        "# 4. Load your Gemma model & tokenizer\n",
        "gemma_checkpoint = \"google/gemma-2b-it\"\n",
        "gemma_tokenizer = AutoTokenizer.from_pretrained(gemma_checkpoint, trust_remote_code=True)\n",
        "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
        "    gemma_checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 5. Load sentences to translate (adapt this line to your source!)\n",
        "with open(\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/BorgiNonModernToModern-hw2_transl-llama.jsonl\", encoding=\"utf-8\") as f:\n",
        "    sentences = [json.loads(line)[\"archaic_sentence\"] for line in f]\n",
        "\n",
        "# 6. Tokenizer padding\n",
        "if gemma_tokenizer.pad_token is None:\n",
        "    gemma_tokenizer.pad_token = gemma_tokenizer.eos_token\n",
        "gemma_tokenizer.padding_side = \"left\"\n",
        "\n",
        "# 7. Build pipeline\n",
        "gemma_translator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=gemma_model,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# 8. Translation (CoT Few-shot) batched\n",
        "batch_size = 8\n",
        "n = len(sentences)\n",
        "gemma_outputs = []\n",
        "\n",
        "total_start = time.time()\n",
        "for start in tqdm(range(0, n, batch_size), desc=\"CoT Few-Shot Translating with Gemma\"):\n",
        "    batch_sentences = sentences[start:start+batch_size]\n",
        "    batch_prompts = [build_cot_fewshot_prompt(s) for s in batch_sentences]\n",
        "    batch_results = gemma_translator(batch_prompts)\n",
        "    for i, r in enumerate(batch_results):\n",
        "        completion = r[0][\"generated_text\"]\n",
        "        result = clean_translation_cot(completion, batch_prompts[i])\n",
        "        gemma_outputs.append(result)\n",
        "total_end = time.time()\n",
        "print(f\"\\nTotal time: {total_end - total_start:.2f} seconds\")\n",
        "print(f\"Average per sentence: {(total_end - total_start)/n:.2f} seconds\")\n",
        "\n",
        "# 9. Save results\n",
        "jsonl_path = \"BorgiNonModernToModern-hw2_transl-gemma-fewshot-cot.jsonl\"\n",
        "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for archaic, modern in zip(sentences, gemma_outputs):\n",
        "        record = {\n",
        "            \"archaic_sentence\": archaic,\n",
        "            \"gemma_translation\": modern\n",
        "        }\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "zMTEMDtBd0mC"
      },
      "id": "zMTEMDtBd0mC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Print 10 random translations\n",
        "with open(jsonl_path, encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "n_samples = min(10, len(data))\n",
        "indices = random.sample(range(len(data)), n_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    print(f\"Archaic Sentence: {data[idx]['archaic_sentence']}\")\n",
        "    print(f\"Gemma Translation: {data[idx]['gemma_translation']}\\n\")"
      ],
      "metadata": {
        "id": "vOaBz6mzevbg"
      },
      "id": "vOaBz6mzevbg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3.2.4: RE-ACT (REASON+ACT) TRANSLATION\n",
        "\n",
        "**TOTAL TIME-TO-RUN:** 298.97 seconds\n",
        "\n",
        "**AVG-per-SENTENCE TIME:** 3.08 seconds"
      ],
      "metadata": {
        "id": "Z_V15331EDNx"
      },
      "id": "Z_V15331EDNx"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1. Define robust ReAct few-shot examples for GEMMA\n",
        "few_shot_react_examples = [\n",
        "    (\n",
        "        \"Ove non sia chi ti conforti, confortati da te stesso.\",\n",
        "        \"La frase sottolinea il valore dell'autosufficienza emotiva. 'Conforti' è un verbo arcaico per 'consolare'. Il messaggio è che bisogna consolarsi da soli se nessuno lo fa per noi.\",\n",
        "        \"Traduci in italiano moderno.\",\n",
        "        \"Se non c'è nessuno a consolarti, consola te stesso.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Così nel suo cammino solingo andava pensoso e lento.\",\n",
        "        \"'Solingo' oggi si direbbe 'solitario'. Descrive una persona che cammina da sola, lentamente, persa nei pensieri.\",\n",
        "        \"Traduci in italiano moderno.\",\n",
        "        \"Così nel suo cammino solitario procedeva pensieroso e lento.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Non è oro tutto quel che riluce.\",\n",
        "        \"È un proverbio che consiglia di non fidarsi delle apparenze; la parola moderna è 'luccica' invece di 'riluce'.\",\n",
        "        \"Traduci in italiano moderno.\",\n",
        "        \"Non è tutto oro ciò che luccica.\"\n",
        "    ),\n",
        "    # (Aggiungi almeno altri 2-3 esempi reali se vuoi massimizzare le performance!)\n",
        "]\n",
        "\n",
        "# 2. Robust ReAct prompt builder\n",
        "def build_react_prompt(archaic_sentence):\n",
        "    intro = (\n",
        "        \"Traduci la seguente frase dall'italiano arcaico all'italiano moderno seguendo il metodo Reason+Act (ReAct). \"\n",
        "        \"Per ciascun esempio, prima ragiona brevemente sul significato e sulle parole arcaiche (Pensiero), poi indica l'azione (Azione) e infine fornisci la traduzione moderna (Traduzione). \"\n",
        "        \"Non riportare gli esempi precedenti. \"\n",
        "        \"Ecco degli esempi:\"\n",
        "    )\n",
        "    shots = \"\\n\".join([\n",
        "        f\"Esempio {i+1}:\\nFrase: {a}\\nPensiero: {thought}\\nAzione: {action}\\nTraduzione: {modern}\"\n",
        "        for i, (a, thought, action, modern) in enumerate(few_shot_react_examples)\n",
        "    ])\n",
        "    test = f\"Esempio {len(few_shot_react_examples)+1}:\\nFrase: {archaic_sentence}\\nPensiero:\"\n",
        "    return f\"{intro}\\n\\n{shots}\\n\\n{test}\"\n",
        "\n",
        "# 3. Improved clean_translation function\n",
        "def clean_translation_react(raw, prompt):\n",
        "    answer = raw.replace(prompt, \"\").strip()\n",
        "    # Find the translation after \"Traduzione:\"\n",
        "    if \"Traduzione:\" in answer:\n",
        "        answer = answer.split(\"Traduzione:\", 1)[-1].strip()\n",
        "    # Cut off generation if model keeps generating more\n",
        "    for stop_word in [\"Esempio\", \"Frase:\", \"Pensiero:\", \"Azione:\", \"\\n\\n\"]:\n",
        "        idx = answer.find(stop_word)\n",
        "        if idx > 1:\n",
        "            answer = answer[:idx].strip()\n",
        "    # Only keep the first line (just in case)\n",
        "    answer = answer.split(\"\\n\")[0].strip()\n",
        "    return answer\n",
        "\n",
        "# 4. Load Gemma model & tokenizer\n",
        "gemma_checkpoint = \"google/gemma-2b-it\"\n",
        "gemma_tokenizer = AutoTokenizer.from_pretrained(gemma_checkpoint, trust_remote_code=True)\n",
        "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
        "    gemma_checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 5. Load sentences to translate\n",
        "with open(\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/BorgiNonModernToModern-hw2_transl-llama.jsonl\", encoding=\"utf-8\") as f:\n",
        "    sentences = [json.loads(line)[\"archaic_sentence\"] for line in f]\n",
        "\n",
        "# 6. Tokenizer padding setup\n",
        "if gemma_tokenizer.pad_token is None:\n",
        "    gemma_tokenizer.pad_token = gemma_tokenizer.eos_token\n",
        "gemma_tokenizer.padding_side = \"left\"\n",
        "\n",
        "# 7. Build translation pipeline\n",
        "gemma_translator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=gemma_model,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# 8. ReAct translation in batch\n",
        "batch_size = 8\n",
        "n = len(sentences)\n",
        "gemma_outputs = []\n",
        "\n",
        "total_start = time.time()\n",
        "for start in tqdm(range(0, n, batch_size), desc=\"ReAct Translating with Gemma\"):\n",
        "    batch_sentences = sentences[start:start+batch_size]\n",
        "    batch_prompts = [build_react_prompt(s) for s in batch_sentences]\n",
        "    batch_results = gemma_translator(batch_prompts)\n",
        "    for i, r in enumerate(batch_results):\n",
        "        completion = r[0][\"generated_text\"]\n",
        "        result = clean_translation_react(completion, batch_prompts[i])\n",
        "        gemma_outputs.append(result)\n",
        "total_end = time.time()\n",
        "print(f\"\\nTotal time: {total_end - total_start:.2f} seconds\")\n",
        "print(f\"Average per sentence: {(total_end - total_start)/n:.2f} seconds\")\n",
        "\n",
        "# 9. Save results\n",
        "jsonl_path = \"BorgiNonModernToModern-hw2_transl-gemma-react.jsonl\"\n",
        "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for archaic, modern in zip(sentences, gemma_outputs):\n",
        "        record = {\n",
        "            \"archaic_sentence\": archaic,\n",
        "            \"gemma_translation\": modern\n",
        "        }\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UT-qdDJmELAT"
      },
      "id": "UT-qdDJmELAT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Print 10 random translations for inspection\n",
        "with open(jsonl_path, encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "n_samples = min(10, len(data))\n",
        "indices = random.sample(range(len(data)), n_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    print(f\"Archaic Sentence: {data[idx]['archaic_sentence']}\")\n",
        "    print(f\"Gemma Translation: {data[idx]['gemma_translation']}\\n\")"
      ],
      "metadata": {
        "id": "YWCPpoOHFO6v"
      },
      "id": "YWCPpoOHFO6v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aGU1s1iunV4-",
      "metadata": {
        "id": "aGU1s1iunV4-"
      },
      "source": [
        "### 4: LLM-AS-A-JUDGE EVALUATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lE7lexZ-cTj9",
      "metadata": {
        "id": "lE7lexZ-cTj9"
      },
      "source": [
        "#### 4.1: GEMINI-2.0-FLASH - GENERAL EVALUATION\n",
        "\n",
        "We will use here Gemini-2.0-Flash Model in a \"General\" setting evaluation. This will simply give us a single score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PDJ3Y0nfdFcb",
      "metadata": {
        "id": "PDJ3Y0nfdFcb"
      },
      "outputs": [],
      "source": [
        "# Information to-set:\n",
        "input_jsonl = \"/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/BorgiNonModernToModern-hw2_transl-nllb.jsonl\"\n",
        "llm_type = \"nllb\"\n",
        "model_name = \"gemini\"\n",
        "judge_type = \"general\"\n",
        "\n",
        "# This mustn't be changed.\n",
        "translation_col = f\"{llm_type}_translation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0oqior3GckEc",
      "metadata": {
        "id": "0oqior3GckEc"
      },
      "outputs": [],
      "source": [
        "# Define the function to run teh Judge.\n",
        "@retry(wait=wait_random_exponential(min=10, max=60), stop=stop_after_attempt(5))\n",
        "def llm_judge_general(sentence, translation):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert evaluator of machine translations from Archaic Italian to Modern Italian.\n",
        "\n",
        "    For each translation, assign a score from 1 (worst) to 5 (best), using this rubric:\n",
        "\n",
        "    1: Completely unacceptable translation. The translation has no pertinence with the original meaning; the generated sentence is either gibberish or makes no sense.\n",
        "    2: Severe semantic errors, omissions or substantial additions on the original sentence. The errors are semantic and syntactic in nature. It’s still something no human would ever write.\n",
        "    3: Partially wrong translation. The translation is lackluster; it contains errors, but mostly minor errors, like typos, or small semantic errors.\n",
        "    4: Good translation. The translation is mostly right, substantially faithful to the original text, but the style does not perfectly match the original sentence; still fluent and comprehensible, and could be semantically acceptable.\n",
        "    5: Perfect translation. The translation is accurate, fluent, complete and coherent. It retained the original meaning as much as it could.\n",
        "\n",
        "    Evaluate ONLY the translation quality according to these guidelines.\n",
        "\n",
        "    Original (Archaic Italian): {sentence}\n",
        "\n",
        "    Translation (Modern Italian): {translation}\n",
        "\n",
        "    Your score (1-5):\n",
        "    \"\"\"\n",
        "    response = LLM_as_a_Judge_model.generate_content(prompt, generation_config={\"temperature\": 0.0})\n",
        "    score_str = response.text.strip()\n",
        "    match = re.search(r\"\\b([1-5])\\b\", score_str)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XzpesVxdoUyu",
      "metadata": {
        "id": "XzpesVxdoUyu"
      },
      "outputs": [],
      "source": [
        "# 1) Configure Gemini API and model.\n",
        "genai.configure(api_key=gemini_api_key)\n",
        "LLM_as_a_Judge_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# 2) Load DataFrame from JSONL.\n",
        "ds = pd.read_json(input_jsonl, lines=True)\n",
        "\n",
        "# 3) Score all translations, pausing every 15 requests (to avoid API rate limits).\n",
        "tqdm.pandas()\n",
        "judge_col = f\"{translation_col}_{judge_type}_judge_score\"\n",
        "\n",
        "scores = []\n",
        "for i, row in tqdm(ds.iterrows(), total=len(ds)):\n",
        "    score = llm_judge_general(row[\"archaic_sentence\"], row[translation_col])\n",
        "    scores.append(score)\n",
        "    if (i + 1) % 15 == 0:\n",
        "        print(\"🕒 Sleeping for 60 seconds to avoid API rate limit...\")\n",
        "        time.sleep(60)\n",
        "\n",
        "ds[judge_col] = scores\n",
        "\n",
        "# Save as JSONL.\n",
        "output_cols = [\"archaic_sentence\", translation_col, judge_col]\n",
        "jsonl_filename = f\"BorgiNonModernToModern-hw2_transl-judge_{model_name}-{judge_type}_{llm_type}-model.jsonl\"\n",
        "with open(jsonl_filename, \"w\", encoding=\"utf8\") as fout:\n",
        "    for record in ds[output_cols].to_dict(orient=\"records\"):\n",
        "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YeUh8khsk2aD",
      "metadata": {
        "id": "YeUh8khsk2aD"
      },
      "source": [
        "#### 4.2: GEMINI-2.0-FLASH - MULTI-CRITERIA EVALUATION\n",
        "\n",
        "We will use here Gemini-2.0-Flash Model in a \"Multi-Criteria\" setting evaluation. This will give us a set of 4 scores for each sentence, evaluating the Adequacy, Fluency, Style and Completeness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EcDu4vYklNX6",
      "metadata": {
        "id": "EcDu4vYklNX6"
      },
      "outputs": [],
      "source": [
        "# Information to-set:\n",
        "input_jsonl = \"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/re-act_prompting/BorgiNonModernToModern-hw2_transl-gemma-fewshot-react.jsonl\"\n",
        "llm_type = \"gemma\"\n",
        "model_name = \"gemini\"\n",
        "judge_type = \"MultiCriteria\"\n",
        "\n",
        "# This mustn't be changed.\n",
        "translation_col = f\"{llm_type}_translation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vJwwtlQdldbC",
      "metadata": {
        "id": "vJwwtlQdldbC"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_random_exponential(min=10, max=60), stop=stop_after_attempt(5))\n",
        "def llm_judge_multicrit(sentence, translation):\n",
        "    prompt = f\"\"\"\n",
        "You are an expert evaluator of machine translations from Archaic Italian to Modern Italian.\n",
        "For each translation, assign a score from 1 (worst) to 5 (best) on the following four criteria. Here is the meaning of each score for each criterion:\n",
        "\n",
        "Adequacy:\n",
        "1 - The translation does not capture the original meaning at all.\n",
        "2 - The translation is mostly wrong; the main meaning is lost, but there are rare fragments of meaning.\n",
        "3 - Some meaning is preserved, but important information is lost or altered.\n",
        "4 - Most meaning is present, with only minor issues; very little is lost.\n",
        "5 - All essential meaning from the original is preserved.\n",
        "\n",
        "Fluency:\n",
        "1 - The translation is unreadable or ungrammatical; clearly machine-generated.\n",
        "2 - The translation has severe grammar errors, unnatural phrasing, or frequent awkwardness.\n",
        "3 - Some awkwardness or minor grammar issues, but still understandable.\n",
        "4 - Mostly fluent and grammatical, only rare awkward or unnatural expressions.\n",
        "5 - Perfectly fluent, fully natural Italian.\n",
        "\n",
        "Style:\n",
        "1 - The tone/register is completely lost or inappropriate.\n",
        "2 - The style is mostly lost; it is awkward or inappropriate for the context.\n",
        "3 - The style is partially preserved but inconsistent or awkward.\n",
        "4 - The style is almost fully preserved, with only minor slips.\n",
        "5 - The style, tone, and register are perfectly matched to the original.\n",
        "\n",
        "Completeness:\n",
        "1 - Major parts are omitted or unnecessary parts are added.\n",
        "2 - The translation is incomplete; many elements are missing or excessive additions present.\n",
        "3 - Minor omissions/additions, but most information is present.\n",
        "4 - Almost everything is present, with only trivial information missing or added.\n",
        "5 - Complete; nothing important is lost or added.\n",
        "\n",
        "Output ONLY the four scores as numbers 1-5, in exactly this format (no extra text):\n",
        "\n",
        "Adequacy: <score>\n",
        "Fluency: <score>\n",
        "Style: <score>\n",
        "Completeness: <score>\n",
        "\n",
        "Original (Archaic Italian): {sentence}\n",
        "\n",
        "Translation (Modern Italian): {translation}\n",
        "    \"\"\"\n",
        "    response = LLM_as_a_Judge_model.generate_content(prompt, generation_config={\"temperature\": 0.0})\n",
        "    text = response.text.strip()\n",
        "    adequacy = re.search(r\"Adequacy:\\s*([1-5])\", text)\n",
        "    fluency = re.search(r\"Fluency:\\s*([1-5])\", text)\n",
        "    style = re.search(r\"Style:\\s*([1-5])\", text)\n",
        "    completeness = re.search(r\"Completeness:\\s*([1-5])\", text)\n",
        "    return {\n",
        "        \"AdequacyScore\": int(adequacy.group(1)) if adequacy else None,\n",
        "        \"FluencyScore\": int(fluency.group(1)) if fluency else None,\n",
        "        \"StyleScore\": int(style.group(1)) if style else None,\n",
        "        \"CompletenessScore\": int(completeness.group(1)) if completeness else None,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MN1fFy84ltzu",
      "metadata": {
        "id": "MN1fFy84ltzu"
      },
      "outputs": [],
      "source": [
        "# 1) Configure Gemini API and model.\n",
        "genai.configure(api_key=gemini_api_key)\n",
        "LLM_as_a_Judge_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# 2) Load DataFrame from JSONL.\n",
        "ds = pd.read_json(input_jsonl, lines=True)\n",
        "\n",
        "# 3) Score all translations, pausing every 15 requests (to avoid API rate limits).\n",
        "tqdm.pandas()\n",
        "judge_col = f\"{translation_col}_{judge_type}_judge_scores\"   # <-- \"_scores\" for dict\n",
        "\n",
        "scores = []\n",
        "for i, row in tqdm(ds.iterrows(), total=len(ds)):\n",
        "    # This returns a dictionary with four scores\n",
        "    score_dict = llm_judge_multicrit(row[\"archaic_sentence\"], row[translation_col])\n",
        "    scores.append(score_dict)\n",
        "    if (i + 1) % 15 == 0:\n",
        "        print(\"🕒 Sleeping for 60 seconds to avoid API rate limit...\")\n",
        "        time.sleep(60)\n",
        "\n",
        "ds[judge_col] = scores\n",
        "\n",
        "# Save as JSONL.\n",
        "output_cols = [\"archaic_sentence\", translation_col, judge_col]\n",
        "jsonl_filename = f\"BorgiNonModernToModern-hw2_transl-judge_{model_name}-{judge_type}_{llm_type}-model.jsonl\"\n",
        "with open(jsonl_filename, \"w\", encoding=\"utf8\") as fout:\n",
        "    for record in ds[output_cols].to_dict(orient=\"records\"):\n",
        "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jD_jz5-D8ALv",
      "metadata": {
        "id": "jD_jz5-D8ALv"
      },
      "source": [
        "#### 4.3: PHI3.5-OPENELM-GEMINI DEBATE-and-CONSENSUS: REFERENCE-FREE SELF-IMPROVING LLM EVALUATION for MACHINE TRANSLATION\n",
        "\n",
        "This evaluation framework introduces a novel, fully reference-free pipeline for scoring machine translations by leveraging the **debate and consensus** paradigm with large language models (LLMs):\n",
        "\n",
        "**MULTI-CRITERIA DUAL-JUDGE SCORING**\n",
        "\n",
        "Each machine translation is independently evaluated by two strong local LLMs, **Phi3.5** and **OpenELM**, according to four established dimensions: adequacy, fluency, style, and completeness. Each judge provides a score from 1 (worst) to 5 (best) for each criterion.\n",
        "\n",
        "**DEBATE & CONSENSUS RESOLUTION**\n",
        "\n",
        "The scores from both judges, along with the original sentence and translation, are then submitted to a third model, **Gemini** (via API). Gemini receives a specialized prompt to \"debate\" the merits of each judge's scores and determine if any criteria should be adjusted. This process simulates expert panel discussion, encouraging self-correction and consensus formation **without the need for gold-standard reference translations**.\n",
        "\n",
        "**ROBUST REFERENCE-FREE EVALUATION**\n",
        "\n",
        "By combining independent perspectives from different models and refining them through debate, this pipeline increases evaluation robustness and mitigates single-model bias. All debate prompts and consensus decisions are logged, providing transparency and a rich resource for future analysis.\n",
        "\n",
        "This approach enables self-improving, reference-free machine translation evaluation, supporting large-scale, reliable assessment of translation quality in scenarios where high-quality human references are unavailable.\n",
        "\n",
        "\n",
        "##### **FURTHER DETAILS**:\n",
        "**PHI-3.5-mini-instruct - Hugging-Face Reference Page:** https://huggingface.co/microsoft/Phi-3.5-mini-instruct\n",
        "\n",
        "**OPENELM-3B-Instruct - Hugging-Face Reference Page:** https://huggingface.co/apple/OpenELM-3B-Instruct"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.1: PHI3.5"
      ],
      "metadata": {
        "id": "QbwR6IxaRL79"
      },
      "id": "QbwR6IxaRL79"
    },
    {
      "cell_type": "code",
      "source": [
        "# — Configuration —\n",
        "INPUT_JSONL     = \"/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/BorgiNonModernToModern-hw2_transl-nllb.jsonl\"\n",
        "llm_type        = \"nllb\"\n",
        "model_name      = \"gemini\"\n",
        "judge_type      = \"phi3.5\"\n",
        "TRANSLATION_COL = f\"{llm_type}_translation\"\n",
        "OUTPUT_JSONL    = f\"BorgiNonModernToModern-hw2_transl-judge_{judge_type}-{llm_type}-model.jsonl\"\n",
        "\n",
        "MULTICRIT_PROMPT = \"\"\"\n",
        "You are an expert evaluator of machine translations from Archaic Italian to Modern Italian.\n",
        "For each translation, assign a score from 1 (worst) to 5 (best) on the following four criteria.\n",
        "\n",
        "Adequacy:\n",
        "1 - The translation does not capture the original meaning at all.\n",
        "2 - The translation is mostly wrong; the main meaning is lost, but there are rare fragments of meaning.\n",
        "3 - Some meaning is preserved, but important information is lost or altered.\n",
        "4 - Most meaning is present, with only minor issues; very little is lost.\n",
        "5 - All essential meaning from the original is preserved.\n",
        "\n",
        "Fluency:\n",
        "1 - The translation is unreadable or ungrammatical; clearly machine-generated.\n",
        "2 - The translation has severe grammar errors, unnatural phrasing, or frequent awkwardness.\n",
        "3 - Some awkwardness or minor grammar issues, but still understandable.\n",
        "4 - Mostly fluent and grammatical, only rare awkward or unnatural expressions.\n",
        "5 - Perfectly fluent, fully natural Italian.\n",
        "\n",
        "Style:\n",
        "1 - The tone/register is completely lost or inappropriate.\n",
        "2 - The style is mostly lost; it is awkward or inappropriate for the context.\n",
        "3 - The style is partially preserved but inconsistent or awkward.\n",
        "4 - The style is almost fully preserved, with only minor slips.\n",
        "5 - The style, tone, and register are perfectly matched to the original.\n",
        "\n",
        "Completeness:\n",
        "1 - Major parts are omitted or unnecessary parts are added.\n",
        "2 - The translation is incomplete; many elements are missing or excessive additions present.\n",
        "3 - Minor omissions/additions, but most information is present.\n",
        "4 - Almost everything is present, with only trivial information missing or added.\n",
        "5 - Complete; nothing important is lost or added.\n",
        "\n",
        "Output ONLY the four scores as numbers 1-5, in exactly this format (no extra text):\n",
        "\n",
        "Adequacy: <score>\n",
        "Fluency: <score>\n",
        "Style: <score>\n",
        "Completeness: <score>\n",
        "\n",
        "Original (Archaic Italian): {sentence}\n",
        "\n",
        "Translation (Modern Italian): {translation}\n",
        "\"\"\"\n",
        "\n",
        "def parse_multicrit_scores(text):\n",
        "    get = lambda label: int(\n",
        "        re.search(rf\"{label}:\\s*([1-5])\", text).group(1)\n",
        "    )\n",
        "    return {\n",
        "        \"Adequacy\":    get(\"Adequacy\"),\n",
        "        \"Fluency\":     get(\"Fluency\"),\n",
        "        \"Style\":       get(\"Style\"),\n",
        "        \"Completeness\":get(\"Completeness\"),\n",
        "    }\n",
        "\n",
        "@retry(wait=wait_random_exponential(10,60), stop=stop_after_attempt(5))\n",
        "def phi3_judge(pipe, sentence, translation):\n",
        "    prompt = MULTICRIT_PROMPT.format(sentence=sentence, translation=translation)\n",
        "    out = pipe(prompt)[0][\"generated_text\"]\n",
        "    return parse_multicrit_scores(out)\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    df = pd.read_json(INPUT_JSONL, lines=True)\n",
        "\n",
        "    # Load Phi-3 pipeline\n",
        "    phi3_ckpt = \"microsoft/phi-3-mini-4k-instruct\"\n",
        "    phi3_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=AutoModelForCausalLM.from_pretrained(\n",
        "            phi3_ckpt,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=\"auto\"\n",
        "        ),\n",
        "        tokenizer=AutoTokenizer.from_pretrained(phi3_ckpt),\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    # Run judgments\n",
        "    results = []\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        sent  = row[\"archaic_sentence\"]\n",
        "        trans = row[TRANSLATION_COL]\n",
        "        try:\n",
        "            scores = phi3_judge(phi3_pipe, sent, trans)\n",
        "        except Exception as e:\n",
        "            print(f\"[{idx}] Judge failed: {e}\")\n",
        "            scores = None\n",
        "        results.append({\n",
        "            \"idx\": idx,\n",
        "            \"archaic_sentence\": sent,\n",
        "            TRANSLATION_COL: trans,\n",
        "            \"phi3_scores\": scores,\n",
        "        })\n",
        "\n",
        "    # Save to JSONL\n",
        "    with open(OUTPUT_JSONL, \"w\", encoding=\"utf8\") as fout:\n",
        "        for rec in results:\n",
        "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "    print(f\"✔ Phi-3 scores written to {OUTPUT_JSONL}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "R85v8eFskPHx"
      },
      "id": "R85v8eFskPHx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.2: OPENELM"
      ],
      "metadata": {
        "id": "vP1cP_VeRTIv"
      },
      "id": "vP1cP_VeRTIv"
    },
    {
      "cell_type": "code",
      "source": [
        "# — Configuration —\n",
        "INPUT_JSONL     = \"/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/BorgiNonModernToModern-hw2_transl-nllb.jsonl\"\n",
        "llm_type        = \"nllb\"\n",
        "judge_type      = \"openELM\"\n",
        "TRANSLATION_COL = f\"{llm_type}_translation\"\n",
        "OUTPUT_JSONL    = f\"BorgiNonModernToModern-hw2_transl-judge_{judge_type}-{llm_type}-model.jsonl\"\n",
        "\n",
        "MULTICRIT_PROMPT = \"\"\"\n",
        "You are an expert evaluator of machine translations from Archaic Italian to Modern Italian.\n",
        "For each translation, assign a score from 1 (worst) to 5 (best) on the following four criteria.\n",
        "\n",
        "Adequacy:\n",
        "1 - The translation does not capture the original meaning at all.\n",
        "2 - The translation is mostly wrong; the main meaning is lost, but there are rare fragments of meaning.\n",
        "3 - Some meaning is preserved, but important information is lost or altered.\n",
        "4 - Most meaning is present, with only minor issues; very little is lost.\n",
        "5 - All essential meaning from the original is preserved.\n",
        "\n",
        "Fluency:\n",
        "1 - The translation is unreadable or ungrammatical; clearly machine-generated.\n",
        "2 - The translation has severe grammar errors, unnatural phrasing, or frequent awkwardness.\n",
        "3 - Some awkwardness or minor grammar issues, but still understandable.\n",
        "4 - Mostly fluent and grammatical, only rare awkward or unnatural expressions.\n",
        "5 - Perfectly fluent, fully natural Italian.\n",
        "\n",
        "Style:\n",
        "1 - The tone/register is completely lost or inappropriate.\n",
        "2 - The style is mostly lost; it is awkward or inappropriate for the context.\n",
        "3 - The style is partially preserved but inconsistent or awkward.\n",
        "4 - The style is almost fully preserved, with only minor slips.\n",
        "5 - The style, tone, and register are perfectly matched to the original.\n",
        "\n",
        "Completeness:\n",
        "1 - Major parts are omitted or unnecessary parts are added.\n",
        "2 - The translation is incomplete; many elements are missing or excessive additions present.\n",
        "3 - Minor omissions/additions, but most information is present.\n",
        "4 - Almost everything is present, with only trivial information missing or added.\n",
        "5 - Complete; nothing important is lost or added.\n",
        "\n",
        "Output ONLY the four scores as numbers 1-5, in exactly this format (no extra text):\n",
        "\n",
        "Adequacy: <score>\n",
        "Fluency: <score>\n",
        "Style: <score>\n",
        "Completeness: <score>\n",
        "\n",
        "Original (Archaic Italian): {sentence}\n",
        "\n",
        "Translation (Modern Italian): {translation}\n",
        "\"\"\"\n",
        "\n",
        "def parse_multicrit_scores(text):\n",
        "    \"\"\"Find the last occurrence of each label in the output and extract its integer score.\"\"\"\n",
        "    scores = {}\n",
        "    for label in [\"Adequacy\", \"Fluency\", \"Style\", \"Completeness\"]:\n",
        "        matches = list(re.finditer(rf\"{label}:\\s*([1-5])\", text))\n",
        "        if not matches:\n",
        "            print(f\"WARNING: Could not find '{label}' in:\\n{text}\\n\")\n",
        "            scores[label] = None\n",
        "        else:\n",
        "            scores[label] = int(matches[-1].group(1))\n",
        "    return scores\n",
        "\n",
        "@retry(wait=wait_random_exponential(10, 60), stop=stop_after_attempt(5))\n",
        "def openelm_judge(pipe, sentence, translation):\n",
        "    prompt = MULTICRIT_PROMPT.format(sentence=sentence, translation=translation)\n",
        "    out = pipe(prompt)[0][\"generated_text\"]\n",
        "    #print(\"the generated output is:\\n\", out)\n",
        "    parsed_output = parse_multicrit_scores(out)\n",
        "    print(\"the parsed output is:\\n\", parsed_output)\n",
        "    return parsed_output\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    df = pd.read_json(INPUT_JSONL, lines=True)\n",
        "\n",
        "    # Load OpenELM pipeline\n",
        "    openelm_ckpt = \"apple/OpenELM-3B-Instruct\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(openelm_ckpt, trust_remote_code=True)\n",
        "    except ValueError:\n",
        "        # Fallback to Llama2 tokenizer if needed\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", trust_remote_code=True)\n",
        "\n",
        "    openelm_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=AutoModelForCausalLM.from_pretrained(\n",
        "            openelm_ckpt,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        ),\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    # Run judgments\n",
        "    results = []\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        sent  = row[\"archaic_sentence\"]\n",
        "        trans = row[TRANSLATION_COL]\n",
        "        try:\n",
        "            scores = openelm_judge(openelm_pipe, sent, trans)\n",
        "        except Exception as e:\n",
        "            print(f\"[{idx}] Judge failed: {e}\")\n",
        "            scores = None\n",
        "        results.append({\n",
        "            \"idx\": idx,\n",
        "            \"archaic_sentence\": sent,\n",
        "            TRANSLATION_COL: trans,\n",
        "            \"openelm_scores\": scores,\n",
        "        })\n",
        "\n",
        "    # Save to JSONL\n",
        "    with open(OUTPUT_JSONL, \"w\", encoding=\"utf8\") as fout:\n",
        "        for rec in results:\n",
        "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "    print(f\"✔ OpenELM scores written to {OUTPUT_JSONL}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "2XA_Uq0Wnjxy"
      },
      "id": "2XA_Uq0Wnjxy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4.3.3: GEMINI DEBATE & CONSENSUS"
      ],
      "metadata": {
        "id": "RaCDG-b3RZPu"
      },
      "id": "RaCDG-b3RZPu"
    },
    {
      "cell_type": "code",
      "source": [
        "# — Configuration —\n",
        "llm_type        = \"nllb\" # or whatever is most correct for your 'translation' field in the file!\n",
        "scores_phi3     = \"/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/eval/multi-criteria-debate&consensus(qwen-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_phi3.5-nllb-model.jsonl\"\n",
        "scores_openelm  = \"/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/eval/multi-criteria-debate&consensus(qwen-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_openELM-nllb-model.jsonl\"\n",
        "output_jsonl    = f\"BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-{llm_type}-model.jsonl\"\n",
        "logs_jsonl      = f\"BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-{llm_type}-logs.jsonl\"\n",
        "\n",
        "# 1) Configure Gemini API and model.\n",
        "genai.configure(api_key=gemini_api_key)\n",
        "\n",
        "def parse_multicrit_scores(text):\n",
        "    get = lambda label: int(re.search(rf\"{label}:\\s*([1-5])\", text).group(1))\n",
        "    return {\n",
        "        \"Adequacy\":    get(\"Adequacy\"),\n",
        "        \"Fluency\":     get(\"Fluency\"),\n",
        "        \"Style\":       get(\"Style\"),\n",
        "        \"Completeness\":get(\"Completeness\"),\n",
        "    }\n",
        "\n",
        "@retry(wait=wait_random_exponential(10, 60), stop=stop_after_attempt(5))\n",
        "def gemini_debate_consensus(sentence, translation, s1, s2):\n",
        "    prompt = f\"\"\"\n",
        "Two expert judges scored this translation independently.\n",
        "\n",
        "Expert 1 scores:\n",
        "Adequacy: {s1['Adequacy']}\n",
        "Fluency: {s1['Fluency']}\n",
        "Style: {s1['Style']}\n",
        "Completeness: {s1['Completeness']}\n",
        "\n",
        "Expert 2 scores:\n",
        "Adequacy: {s2['Adequacy']}\n",
        "Fluency: {s2['Fluency']}\n",
        "Style: {s2['Style']}\n",
        "Completeness: {s2['Completeness']}\n",
        "\n",
        "Original (Archaic Italian): {sentence}\n",
        "Translation (Modern Italian): {translation}\n",
        "\n",
        "Please debate which scores are most accurate and, if any should change,\n",
        "output ONLY the final four scores in exactly this format (no extra text):\n",
        "\n",
        "Adequacy: <1–5>\n",
        "Fluency: <1–5>\n",
        "Style: <1–5>\n",
        "Completeness: <1–5>\n",
        "\"\"\"\n",
        "    resp = genai.GenerativeModel(\"gemini-2.0-flash\").generate_content(prompt, generation_config={\"temperature\": 0.0})\n",
        "    text = resp.text.strip()\n",
        "    return parse_multicrit_scores(text), prompt, text\n",
        "\n",
        "def main():\n",
        "    # Load the Phi3 & OpenELM score files into dicts keyed by idx\n",
        "    phi3_data    = {r[\"idx\"]: r for r in map(json.loads, open(scores_phi3, encoding=\"utf8\"))}\n",
        "    openelm_data = {r[\"idx\"]: r for r in map(json.loads, open(scores_openelm, encoding=\"utf8\"))}\n",
        "\n",
        "    final_records = []\n",
        "    debate_logs   = []\n",
        "\n",
        "    for idx in sorted(phi3_data):\n",
        "        rec1 = phi3_data[idx]\n",
        "        rec2 = openelm_data[idx]\n",
        "\n",
        "        sent  = rec1[\"archaic_sentence\"]\n",
        "        trans = rec1[f\"{llm_type}_translation\"]\n",
        "        s1    = rec1[\"phi3_scores\"]\n",
        "        s2    = rec2[\"openelm_scores\"]\n",
        "\n",
        "        consensus, prompt, response = gemini_debate_consensus(sent, trans, s1, s2)\n",
        "        changed = (consensus != s1 and consensus != s2)\n",
        "        print(\"The consesuns from Gemini is: \", consensus)\n",
        "        final_records.append({\n",
        "            \"idx\":                  idx,\n",
        "            \"archaic_sentence\":     sent,\n",
        "            f\"{llm_type}_translation\": trans,\n",
        "            \"phi3_scores\":          s1,\n",
        "            \"openelm_scores\":       s2,\n",
        "            \"gemini_debate_consensus\":     consensus,\n",
        "            \"debate_model\":         \"gemini\",\n",
        "            \"debate_changed\":       changed,\n",
        "        })\n",
        "        debate_logs.append({\n",
        "            \"idx\":             idx,\n",
        "            \"prompt\":          prompt,\n",
        "            \"response\":        response,\n",
        "            \"consensus_scores\": consensus,\n",
        "        })\n",
        "\n",
        "        # throttle to respect rate limits\n",
        "        if (idx + 1) % 15 == 0:\n",
        "            time.sleep(60)\n",
        "\n",
        "    # Write out the final judgments\n",
        "    with open(output_jsonl, \"w\", encoding=\"utf8\") as fout:\n",
        "        for rec in final_records:\n",
        "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    # Write out the debate logs\n",
        "    with open(logs_jsonl, \"w\", encoding=\"utf8\") as fout:\n",
        "        for log in debate_logs:\n",
        "            fout.write(json.dumps(log, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"✔ Debate & consensus written to {output_jsonl}\")\n",
        "    print(f\"✔ Logs written to {logs_jsonl}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ccuNyH3U7Nlu"
      },
      "id": "ccuNyH3U7Nlu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5: GOLD-LABELS & LLM-as-a-JUDGE CORRELATION"
      ],
      "metadata": {
        "id": "GaCsILdwpXH3"
      },
      "id": "GaCsILdwpXH3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1: SINGLE-CRITERIA\n",
        "\n",
        "Now we decide to compute the correlations w.r.t. the single criterion among the models.\n",
        "\n",
        "Indeed, every row in the output file is the result of comparing the criterion between the gold labels and a judge's predictions for one model/judge file.\n",
        "(Total # lines = 10 --> 10 models * 1 criteria)"
      ],
      "metadata": {
        "id": "i8bUcU9aZJtZ"
      },
      "id": "i8bUcU9aZJtZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "\n",
        "def read_jsonl_scores(file_path, score_field, key_field=\"archaic_sentence\", n=None):\n",
        "    data = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n and i >= n:\n",
        "                break\n",
        "            row = json.loads(line)\n",
        "            data.append((row[key_field], row[score_field]))\n",
        "    return dict(data)\n",
        "\n",
        "def compute_metrics(gold_scores, judge_scores):\n",
        "    results = {}\n",
        "    results['cohen_kappa'] = cohen_kappa_score(gold_scores, judge_scores)\n",
        "    results['pearson'], _  = pearsonr(gold_scores, judge_scores)\n",
        "    results['spearman'], _ = spearmanr(gold_scores, judge_scores)\n",
        "    results['kendall'], _  = kendalltau(gold_scores, judge_scores)\n",
        "    results['exact_match'] = accuracy_score(gold_scores, judge_scores)\n",
        "    results['confusion_matrix'] = confusion_matrix(gold_scores, judge_scores)\n",
        "    return results\n",
        "\n",
        "def main(gold_jsonl, judge_gold_mapping, n=None):\n",
        "    # Load gold file once\n",
        "    with open(gold_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
        "        gold_rows = [json.loads(line) for line in f]\n",
        "    # Use \"archaic_sentence\" as key\n",
        "    gold_dict = {row[\"archaic_sentence\"]: row for row in gold_rows}\n",
        "\n",
        "    rows = []\n",
        "    for judge_path, gold_field, judge_field in judge_gold_mapping:\n",
        "        # Read judge file\n",
        "        judge_dict = read_jsonl_scores(judge_path, judge_field, n=n)\n",
        "        # Build matching lists\n",
        "        gold_list, judge_list = [], []\n",
        "        for k in judge_dict:\n",
        "            if k in gold_dict and gold_field in gold_dict[k]:\n",
        "                gold_list.append(gold_dict[k][gold_field])\n",
        "                judge_list.append(judge_dict[k])\n",
        "        # Convert to int (or float if needed)\n",
        "        gold_list = list(map(int, gold_list))\n",
        "        judge_list = list(map(int, judge_list))\n",
        "        if not gold_list:\n",
        "            print(f\"Warning: no matching sentences for {judge_path} and gold field {gold_field}\")\n",
        "            continue\n",
        "        metrics = compute_metrics(gold_list, judge_list)\n",
        "        row = {\n",
        "            'judge_file': judge_path,\n",
        "            'gold_field': gold_field,\n",
        "            'judge_field': judge_field,\n",
        "            'cohen_kappa': metrics['cohen_kappa'],\n",
        "            'pearson': metrics['pearson'],\n",
        "            'spearman': metrics['spearman'],\n",
        "            'kendall': metrics['kendall'],\n",
        "            'exact_match': metrics['exact_match'],\n",
        "            'n': len(gold_list)\n",
        "        }\n",
        "        rows.append(row)\n",
        "        print(f\"\\nConfusion matrix for {judge_path} (gold={gold_field}):\\n\", metrics['confusion_matrix'])\n",
        "    # Print summary\n",
        "    df = pd.DataFrame(rows)\n",
        "    print(\"\\nSummary Table:\\n\")\n",
        "    print(df)\n",
        "    with open(\"gold-labels_&_judge-labels_singlecriteria-correlations_summary.jsonl\", \"w\", encoding=\"utf-8\") as fout:\n",
        "      for row in rows:\n",
        "          json.dump(row, fout)\n",
        "          fout.write('\\n')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gold_jsonl = \"/content/drive/MyDrive/BorgiNonModernToModern/gold_labels_SingleCriteria_first30s.jsonl\"\n",
        "\n",
        "    # (GENERAL / SINGLE CRITERIA)\n",
        "    # The tuples here are: (judge_file, gold_field, judge_field)\n",
        "    judge_gold_mapping = [\n",
        "\n",
        "        ### Transformer-based ###\n",
        "        # Transformer-based - Single-Criteria - mBART\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_mbart-model.jsonl\", \"mbart_score\", \"mbart_translation_general_judge_score\"),\n",
        "        # Transformer-based - Single-Criteria - NLLB\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_nllb-model.jsonl\", \"nllb_score\", \"nllb_translation_general_judge_score\"),\n",
        "\n",
        "        ### LLM-based ###\n",
        "        # LLM-based - Single-Criteria - Zero-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_gemma-model.jsonl\", \"gemma_translation_zero-shot_score\", \"gemma_translation_general_judge_score\"),\n",
        "        # LLM-based - Single-Criteria - Zero-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_llama-model.jsonl\", \"llama_translation_zero-shot_score\", \"llama_translation_general_judge_score\"),\n",
        "\n",
        "        # LLM-based - Single-Criteria - Few-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/few-shot_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_gemma-model.jsonl\", \"gemma_translation_fewshot_score\", \"gemma_translation_general_judge_score\"),\n",
        "        # LLM-based - Single-Criteria - Few-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/few-shot_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_llama-model.jsonl\", \"llama_translation_fewshot_score\", \"llama_translation_general_judge_score\"),\n",
        "\n",
        "        # LLM-based - Single-Criteria - Few-Shot-CoT-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/chain_of_thoughts_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_gemma-model.jsonl\", \"gemma_translation_fewshot-cot_score\", \"gemma_translation_general_judge_score\"),\n",
        "        # LLM-based - Single-Criteria - Few-Shot-CoT-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/chain_of_thoughts_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_llama-model.jsonl\", \"llama_translation_fewshot-cot_score\", \"llama_translation_general_judge_score\"),\n",
        "\n",
        "        # LLM-based - Single-Criteria - Few-Shot-ReAct-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/re-act_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_gemma-model.jsonl\", \"gemma_translation_fewshot-react_score\", \"gemma_translation_general_judge_score\"),\n",
        "        # LLM-based - Single-Criteria - Few-Shot-ReAct-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/re-act_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_llama-model.jsonl\", \"llama_translation_fewshot-react_score\", \"llama_translation_general_judge_score\"),\n",
        "    ]\n",
        "    n = 30  # Set None for all lines, or an integer for the first N\n",
        "    main(gold_jsonl, judge_gold_mapping, n=n)\n"
      ],
      "metadata": {
        "id": "T4qq4FAdpkFw"
      },
      "id": "T4qq4FAdpkFw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2: MULTI-CRITERIA\n",
        "\n",
        "Now we decide to compute the correlations w.r.t. 4 different criterion among the models.\n",
        "\n",
        "Indeed, every row in the output file is the result of comparing one criterion between the gold labels and a judge's predictions for one model/judge file.\n",
        "(Total # lines = 40 --> 10 models * 4 different criteria)"
      ],
      "metadata": {
        "id": "1yv5rAphZN1A"
      },
      "id": "1yv5rAphZN1A"
    },
    {
      "cell_type": "code",
      "source": [
        "def read_jsonl_multi_scores(file_path, score_field, key_field=\"archaic_sentence\", n=None):\n",
        "    \"\"\"\n",
        "    Reads a JSONL file and returns a dict mapping key_field → scores dict.\n",
        "    If the score_field is 'gemini_debate_consensus', normalize its keys\n",
        "    by appending 'Score' to match your multi-criteria names.\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n and i >= n:\n",
        "                break\n",
        "            row = json.loads(line)\n",
        "            scores = row[score_field]\n",
        "            # --- normalize the debate&consensus keys to end with 'Score' ----\n",
        "            if score_field == \"gemini_debate_consensus\" and isinstance(scores, dict):\n",
        "                scores = { f\"{k}Score\": v for k, v in scores.items() }\n",
        "            data[row[key_field]] = scores\n",
        "    return data\n",
        "\n",
        "def compute_metrics(gold_scores, judge_scores):\n",
        "    \"\"\"Computes a suite of agreement/correlation metrics.\"\"\"\n",
        "    results = {}\n",
        "    try:\n",
        "        results['cohen_kappa'] = cohen_kappa_score(gold_scores, judge_scores)\n",
        "    except Exception as e:\n",
        "        results['cohen_kappa'] = str(e)\n",
        "    try:\n",
        "        results['pearson'], _  = pearsonr(gold_scores, judge_scores)\n",
        "    except Exception as e:\n",
        "        results['pearson'] = str(e)\n",
        "    try:\n",
        "        results['spearman'], _ = spearmanr(gold_scores, judge_scores)\n",
        "    except Exception as e:\n",
        "        results['spearman'] = str(e)\n",
        "    try:\n",
        "        results['kendall'], _  = kendalltau(gold_scores, judge_scores)\n",
        "    except Exception as e:\n",
        "        results['kendall'] = str(e)\n",
        "    try:\n",
        "        results['exact_match'] = accuracy_score(gold_scores, judge_scores)\n",
        "    except Exception as e:\n",
        "        results['exact_match'] = str(e)\n",
        "    try:\n",
        "        results['confusion_matrix'] = confusion_matrix(gold_scores, judge_scores).tolist()\n",
        "    except Exception as e:\n",
        "        results['confusion_matrix'] = str(e)\n",
        "    return results\n",
        "\n",
        "def main(gold_jsonl, judge_gold_mapping, criteria_list=None, n=None):\n",
        "    # Load gold file once\n",
        "    with open(gold_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
        "        gold_rows = [json.loads(line) for line in f]\n",
        "    gold_dict = {row[\"archaic_sentence\"]: row for row in gold_rows}\n",
        "\n",
        "    # If no criteria_list given, infer from the first dict-valued field.\n",
        "    if criteria_list is None:\n",
        "        for row in gold_rows:\n",
        "            for v in row.values():\n",
        "                if isinstance(v, dict):\n",
        "                    criteria_list = list(v.keys())\n",
        "                    break\n",
        "            if criteria_list:\n",
        "                break\n",
        "        if criteria_list is None:\n",
        "            raise ValueError(\"Could not infer criteria fields; please specify criteria_list.\")\n",
        "    print(\"Evaluating criteria:\", criteria_list)\n",
        "\n",
        "    rows = []\n",
        "    for judge_path, gold_field, judge_field in judge_gold_mapping:\n",
        "        print(f\"\\n▶ Reading {judge_field} from {judge_path}\")\n",
        "        judge_dict = read_jsonl_multi_scores(judge_path, judge_field, n=n)\n",
        "\n",
        "        for criterion in criteria_list:\n",
        "            gold_list, judge_list = [], []\n",
        "            for key, j_scores in judge_dict.items():\n",
        "                if ( key in gold_dict\n",
        "                     and gold_field in gold_dict[key]\n",
        "                     and isinstance(gold_dict[key][gold_field], dict)\n",
        "                     and criterion in gold_dict[key][gold_field]\n",
        "                     and isinstance(j_scores, dict)\n",
        "                     and criterion in j_scores ):\n",
        "                    gold_list.append( gold_dict[key][gold_field][criterion] )\n",
        "                    judge_list.append( j_scores[criterion] )\n",
        "\n",
        "            # cast to int if possible.\n",
        "            try:\n",
        "                gold_list  = list(map(int, gold_list))\n",
        "                judge_list = list(map(int, judge_list))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            if not gold_list:\n",
        "                print(f\"No matching data for {judge_field}, criterion={criterion}\")\n",
        "                continue\n",
        "\n",
        "            metrics = compute_metrics(gold_list, judge_list)\n",
        "            rows.append({\n",
        "                'judge_file':   judge_path,\n",
        "                'gold_field':   gold_field,\n",
        "                'judge_field':  judge_field,\n",
        "                'criterion':    criterion,\n",
        "                'cohen_kappa':  metrics['cohen_kappa'],\n",
        "                'pearson':      metrics['pearson'],\n",
        "                'spearman':     metrics['spearman'],\n",
        "                'kendall':      metrics['kendall'],\n",
        "                'exact_match':  metrics['exact_match'],\n",
        "                'n':            len(gold_list)\n",
        "            })\n",
        "            print(f\"  • {criterion}: CM = {metrics['confusion_matrix']}\")\n",
        "\n",
        "    # summary.\n",
        "    df = pd.DataFrame(rows)\n",
        "    print(\"\\nSummary:\\n\", df)\n",
        "\n",
        "    # save.\n",
        "    out_path = \"gold-labels_&_judge-labels_multicriteria-correlations_summary.jsonl\"\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "        for r in rows:\n",
        "            json.dump(r, fout)\n",
        "            fout.write(\"\\n\")\n",
        "    print(f\"\\n✔ Wrote summary to {out_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gold_jsonl = \"/content/drive/MyDrive/BorgiNonModernToModern/gold_labels_MultiCriteria_first30s.jsonl\"\n",
        "    # MULTI CRITERIA)\n",
        "    # The tuples here are: (judge_file, gold_field, judge_field)\n",
        "    judge_gold_mapping = [\n",
        "\n",
        "        ### Transformer-based ###\n",
        "        # Transformer-based - Multi-Criteria - mBART\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/eval/multi-criteria/BorgiNonModernToModern-hw2_transl-judge_gemini-MultiCriteria_mbart-model.jsonl\", \"mbart_gold_label_scores\", \"mbart_translation_MultiCriteria_judge_scores\"),\n",
        "        # Transformer-based - Multi-Criteria - NLLB\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/eval/multi-criteria/BorgiNonModernToModern-hw2_transl-judge_gemini-MultiCriteria_nllb-model.jsonl\", \"nllb_gold_label_scores\", \"nllb_translation_MultiCriteria_judge_scores\"),\n",
        "        # Transformer-based - Multi-Criteria Debate&Consensus - mBART\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/eval/multi-criteria-debate&consensus(phi-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-mbart-model.jsonl\", \"mbart_gold_label_scores\", \"gemini_debate_consensus\"),\n",
        "        # Transformer-based - Multi-Criteria Debate&Consensus - NLLB\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/eval/multi-criteria-debate&consensus(phi-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-nllb-model.jsonl\", \"nllb_gold_label_scores\", \"gemini_debate_consensus\"),\n",
        "\n",
        "\n",
        "        ### LLM-based ###\n",
        "        # LLM-based - Multi-Criteria - Zero-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/eval/multi-criteria/BorgiNonModernToModern-hw2_transl-judge_gemini-MultiCriteria_gemma-model.jsonl\", \"gemma_translation_zero-shot_gold_label_scores\", \"gemma_translation_MultiCriteria_judge_scores\"),\n",
        "        # LLM-based - Multi-Criteria - Zero-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/eval/multi-criteria/BorgiNonModernToModern-hw2_transl-judge_gemini-MultiCriteria_llama-model.jsonl\", \"llama_translation_zero-shot_gold_label_scores\", \"llama_translation_MultiCriteria_judge_scores\"),\n",
        "\n",
        "        # LLM-based - Multi-Criteria - Few-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/few-shot_prompting/eval/multi-criteria/BorgiNonModernToModern-hw2_transl-judge_gemini-MultiCriteria_gemma-model.jsonl\", \"gemma_translation_fewshot_gold_label_scores\", \"gemma_translation_MultiCriteria_judge_scores\"),\n",
        "        # LLM-based - Multi-Criteria - Few-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/few-shot_prompting/eval/multi-criteria/BorgiNonModernToModern-hw2_transl-judge_gemini-MultiCriteria_llama-model.jsonl\", \"llama_translation_fewshot_gold_label_scores\", \"llama_translation_MultiCriteria_judge_scores\"),\n",
        "\n",
        "        # LLM-based - Multi-Criteria - Few-Shot-CoT-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/chain_of_thoughts_prompting/eval/multi-criteria/BorgiNonModernToModern-hw2_transl-judge_gemini-MultiCriteria_gemma-model.jsonl\", \"gemma_translation_fewshot-cot_gold_label_scores\", \"gemma_translation_MultiCriteria_judge_scores\"),\n",
        "        # LLM-based - Multi-Criteria - Few-Shot-CoT-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/chain_of_thoughts_prompting/eval/multi-criteria/BorgiNonModernToModern-hw2_transl-judge_gemini-MultiCriteria_llama-model.jsonl\", \"llama_translation_fewshot-cot_gold_label_scores\", \"llama_translation_MultiCriteria_judge_scores\"),\n",
        "\n",
        "        # LLM-based - Multi-Criteria - Few-Shot-ReAct-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/re-act_prompting/eval/multi-criteria/BorgiNonModernToModern-hw2_transl-judge_gemini-MultiCriteria_gemma-model.jsonl\", \"gemma_translation_fewshot-react_gold_label_scores\", \"gemma_translation_MultiCriteria_judge_scores\"),\n",
        "        # LLM-based - Multi-Criteria - Few-Shot-ReAct-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/re-act_prompting/eval/multi-criteria/BorgiNonModernToModern-hw2_transl-judge_gemini-MultiCriteria_llama-model.jsonl\", \"llama_translation_fewshot-react_gold_label_scores\", \"llama_translation_MultiCriteria_judge_scores\"),\n",
        "\n",
        "\n",
        "\n",
        "        # LLM-based - Multi-Criteria Debate&Consensus - Zero-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/eval/multi-criteria-debate&consensus(phi-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-gemma-model.jsonl\", \"gemma_translation_zero-shot_gold_label_scores\", \"gemini_debate_consensus\"),\n",
        "        # LLM-based - Multi-Criteria - Zero-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/eval/multi-criteria-debate&consensus(phi-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-llama-model.jsonl\", \"llama_translation_zero-shot_gold_label_scores\", \"gemini_debate_consensus\"),\n",
        "\n",
        "        # LLM-based - Multi-Criteria Debate&Consensus - Few-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/few-shot_prompting/eval/multi-criteria-debate&consensus(phi-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-gemma-model.jsonl\", \"gemma_translation_fewshot_gold_label_scores\", \"gemini_debate_consensus\"),\n",
        "        # LLM-based - Multi-Criteria Debate&Consensus - Few-Shot-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/few-shot_prompting/eval/multi-criteria-debate&consensus(phi-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-llama-model.jsonl\", \"llama_translation_fewshot_gold_label_scores\", \"gemini_debate_consensus\"),\n",
        "\n",
        "        # LLM-based - Multi-Criteria Debate&Consensus - Few-Shot-CoT-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/chain_of_thoughts_prompting/eval/multi-criteria-debate&consensus(phi-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-gemma-model.jsonl\", \"gemma_translation_fewshot-cot_gold_label_scores\", \"gemini_debate_consensus\"),\n",
        "        # LLM-based - Multi-Criteria Debate&Consensus - Few-Shot-CoT-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/chain_of_thoughts_prompting/eval/multi-criteria-debate&consensus(phi-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-llama-model.jsonl\", \"llama_translation_fewshot-cot_gold_label_scores\", \"gemini_debate_consensus\"),\n",
        "\n",
        "        # LLM-based - Multi-Criteria Debate&Consensus - Few-Shot-ReAct-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/re-act_prompting/eval/multi-criteria-debate&consensus(phi-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-gemma-model.jsonl\", \"gemma_translation_fewshot-react_gold_label_scores\", \"gemini_debate_consensus\"),\n",
        "        # LLM-based - Multi-Criteria Debate&Consensus - Few-Shot-ReAct-Prompting\n",
        "        (\"/content/drive/MyDrive/BorgiNonModernToModern/llm_based/re-act_prompting/eval/multi-criteria-debate&consensus(phi-openelm-gemini)/BorgiNonModernToModern-hw2_transl-judge_DebateConsensusPhiOpenELMGemini-llama-model.jsonl\", \"llama_translation_fewshot-react_gold_label_scores\", \"gemini_debate_consensus\"),\n",
        "\n",
        "    ]\n",
        "    # These are your sub-criteria (update if needed)\n",
        "    criteria_list = [\"AdequacyScore\", \"FluencyScore\", \"StyleScore\", \"CompletenessScore\"]\n",
        "    n = 30  # Set None for all lines, or an integer for the first N\n",
        "    main(gold_jsonl, judge_gold_mapping, criteria_list=criteria_list, n=n)\n"
      ],
      "metadata": {
        "id": "ATITWlFkdR-F"
      },
      "id": "ATITWlFkdR-F",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aa355adc8ac148eeae1b026ab3cf8660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8245971a57a2499790827e8b5c48dd72",
              "IPY_MODEL_74993adca5834498ba9bfa99139ab70e",
              "IPY_MODEL_b78ac95ee46743299eab05d9ad0739e4"
            ],
            "layout": "IPY_MODEL_4459082f1f7b48ea9bcdef7617c3f292"
          }
        },
        "8245971a57a2499790827e8b5c48dd72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83f2f49a62df4224b62f987119f66268",
            "placeholder": "​",
            "style": "IPY_MODEL_c55dd872b6cf4a518b4fcb2a63ecfb05",
            "value": "README.md: 100%"
          }
        },
        "74993adca5834498ba9bfa99139ab70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d4e06ac903c42ff8c19aa3ef3f40f85",
            "max": 370,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_107de5ecf8264cfcaab5072f68613ba7",
            "value": 370
          }
        },
        "b78ac95ee46743299eab05d9ad0739e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3130fc0a8c04d74819daf7d1f73c242",
            "placeholder": "​",
            "style": "IPY_MODEL_1efca8fd049c41b8ae0ade9687682dc7",
            "value": " 370/370 [00:00&lt;00:00, 36.9kB/s]"
          }
        },
        "4459082f1f7b48ea9bcdef7617c3f292": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f2f49a62df4224b62f987119f66268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c55dd872b6cf4a518b4fcb2a63ecfb05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d4e06ac903c42ff8c19aa3ef3f40f85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "107de5ecf8264cfcaab5072f68613ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3130fc0a8c04d74819daf7d1f73c242": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1efca8fd049c41b8ae0ade9687682dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bdd1a40feb4450e878ae82af0d9ff42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fc5c3ad5eff455cbbfd75390f8b0351",
              "IPY_MODEL_15b504d56e794ec689fb933572b5ee50",
              "IPY_MODEL_4b18c578aeb444edae4ba3f1c2d537a1"
            ],
            "layout": "IPY_MODEL_900254183f6e4b27a32280e6f27877c7"
          }
        },
        "1fc5c3ad5eff455cbbfd75390f8b0351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8848c67c9b004216979e609ae6d5d22b",
            "placeholder": "​",
            "style": "IPY_MODEL_85c84ffbd6d542268a38eb2676f28d99",
            "value": "test-00000-of-00001.parquet: 100%"
          }
        },
        "15b504d56e794ec689fb933572b5ee50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8e9b0b5332b4526a155c0f65a493a50",
            "max": 11785,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f16a3be7e8cd404ba07dd77a8e12fdea",
            "value": 11785
          }
        },
        "4b18c578aeb444edae4ba3f1c2d537a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c1379ebeb8c4b9ea9e88cffc48fdf64",
            "placeholder": "​",
            "style": "IPY_MODEL_d0b39251d711483eb8c5defbbe1a8f53",
            "value": " 11.8k/11.8k [00:00&lt;00:00, 1.37MB/s]"
          }
        },
        "900254183f6e4b27a32280e6f27877c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8848c67c9b004216979e609ae6d5d22b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85c84ffbd6d542268a38eb2676f28d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8e9b0b5332b4526a155c0f65a493a50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f16a3be7e8cd404ba07dd77a8e12fdea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c1379ebeb8c4b9ea9e88cffc48fdf64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0b39251d711483eb8c5defbbe1a8f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3c52ec3cb8f4cd79166c4441f9c6c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f13639f7cc9644e8825e564587260913",
              "IPY_MODEL_3564528245bf430eae0f0d5ba1b29f16",
              "IPY_MODEL_315fb964ceac43bfb8204d5b10005d3f"
            ],
            "layout": "IPY_MODEL_8bcd77cb09c74024bedaacccef6715dc"
          }
        },
        "f13639f7cc9644e8825e564587260913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c46ac2bc8294457b87accab57adf0206",
            "placeholder": "​",
            "style": "IPY_MODEL_851b0f6af72c45fc9acabffb002ba902",
            "value": "Generating test split: 100%"
          }
        },
        "3564528245bf430eae0f0d5ba1b29f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62b6265f9cc74f1db86aed537cc97e5f",
            "max": 97,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35aa1062e0964c6e9cb44bcc04d61df3",
            "value": 97
          }
        },
        "315fb964ceac43bfb8204d5b10005d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feed346719bf4d65b7d40772778f138b",
            "placeholder": "​",
            "style": "IPY_MODEL_735fae7cca2043e5a078cec25da97df8",
            "value": " 97/97 [00:00&lt;00:00, 2554.90 examples/s]"
          }
        },
        "8bcd77cb09c74024bedaacccef6715dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c46ac2bc8294457b87accab57adf0206": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "851b0f6af72c45fc9acabffb002ba902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62b6265f9cc74f1db86aed537cc97e5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35aa1062e0964c6e9cb44bcc04d61df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "feed346719bf4d65b7d40772778f138b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "735fae7cca2043e5a078cec25da97df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}