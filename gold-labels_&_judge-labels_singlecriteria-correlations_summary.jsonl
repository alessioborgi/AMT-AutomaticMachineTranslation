{"judge_file": "/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_mbart-model.jsonl", "gold_field": "mbart_score", "judge_field": "mbart_translation_general_judge_score", "cohen_kappa": 0.3497109826589595, "pearson": 0.6594818055702395, "spearman": 0.6900182665330173, "kendall": 0.6441937525140228, "exact_match": 0.5, "n": 30}
{"judge_file": "/content/drive/MyDrive/BorgiNonModernToModern/transformer_based/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_nllb-model.jsonl", "gold_field": "nllb_score", "judge_field": "nllb_translation_general_judge_score", "cohen_kappa": 0.2558139534883721, "pearson": 0.5733191476030296, "spearman": 0.6344898527257654, "kendall": 0.5574356135706044, "exact_match": 0.4666666666666667, "n": 30}
{"judge_file": "/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_gemma-model.jsonl", "gold_field": "gemma_translation_zero-shot_score", "judge_field": "gemma_translation_general_judge_score", "cohen_kappa": 0.25233644859813076, "pearson": 0.31596875932547985, "spearman": 0.4244525428095125, "kendall": 0.35793417881612083, "exact_match": 0.4666666666666667, "n": 30}
{"judge_file": "/content/drive/MyDrive/BorgiNonModernToModern/llm_based/zero-shot_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_llama-model.jsonl", "gold_field": "llama_translation_zero-shot_score", "judge_field": "llama_translation_general_judge_score", "cohen_kappa": 0.4809688581314878, "pearson": 0.7758432778320212, "spearman": 0.7765003391543324, "kendall": 0.7241679221261326, "exact_match": 0.6666666666666666, "n": 30}
{"judge_file": "/content/drive/MyDrive/BorgiNonModernToModern/llm_based/few-shot_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_gemma-model.jsonl", "gold_field": "gemma_translation_fewshot_score", "judge_field": "gemma_translation_general_judge_score", "cohen_kappa": 0.031100478468899517, "pearson": 0.2952909296357579, "spearman": 0.22085730634329367, "kendall": 0.20486298428816546, "exact_match": 0.1, "n": 30}
{"judge_file": "/content/drive/MyDrive/BorgiNonModernToModern/llm_based/few-shot_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_llama-model.jsonl", "gold_field": "llama_translation_fewshot_score", "judge_field": "llama_translation_general_judge_score", "cohen_kappa": 0.25399644760213136, "pearson": 0.6915068435965432, "spearman": 0.662589381239177, "kendall": 0.6050764353498551, "exact_match": 0.5333333333333333, "n": 30}
{"judge_file": "/content/drive/MyDrive/BorgiNonModernToModern/llm_based/chain_of_thoughts_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_gemma-model.jsonl", "gold_field": "gemma_translation_fewshot-cot_score", "judge_field": "gemma_translation_general_judge_score", "cohen_kappa": 0.08200734394124853, "pearson": 0.31414405555298786, "spearman": 0.32095304468572056, "kendall": 0.2852090691437395, "exact_match": 0.16666666666666666, "n": 30}
{"judge_file": "/content/drive/MyDrive/BorgiNonModernToModern/llm_based/chain_of_thoughts_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_llama-model.jsonl", "gold_field": "llama_translation_fewshot-cot_score", "judge_field": "llama_translation_general_judge_score", "cohen_kappa": -0.01941747572815533, "pearson": 0.02928601137069234, "spearman": 0.0996493708514786, "kendall": 0.09389842426831226, "exact_match": 0.3, "n": 30}
{"judge_file": "/content/drive/MyDrive/BorgiNonModernToModern/llm_based/re-act_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_gemma-model.jsonl", "gold_field": "gemma_translation_fewshot-react_score", "judge_field": "gemma_translation_general_judge_score", "cohen_kappa": 0.07096774193548394, "pearson": 0.472277492873571, "spearman": 0.4977596087949616, "kendall": 0.44553693022662166, "exact_match": 0.2, "n": 30}
{"judge_file": "/content/drive/MyDrive/BorgiNonModernToModern/llm_based/re-act_prompting/eval/general/BorgiNonModernToModern-hw2_transl-judge_gemini-general_llama-model.jsonl", "gold_field": "llama_translation_fewshot-react_score", "judge_field": "llama_translation_general_judge_score", "cohen_kappa": 0.2415730337078652, "pearson": 0.6500406178891796, "spearman": 0.6721377636595418, "kendall": 0.5793668816963543, "exact_match": 0.4, "n": 30}
