{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0051ba",
   "metadata": {},
   "source": [
    "# AMT - AUTOMATIC MACHINE TRANSLATION\n",
    "\n",
    "@alessioborgi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5815e93",
   "metadata": {},
   "source": [
    "### 0: IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde1395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.9/site-packages (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.9/site-packages (0.31.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.9/site-packages (4.52.3)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.9/site-packages (6.31.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.9/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.9/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.9/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.9/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub) (4.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.9/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.9/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Downloading sentencepiece-0.2.0-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m811.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets huggingface-hub pandas transformers datasets tiktoken protobuf sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd111d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for step 1).\n",
    "import os\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Importing libraries for step 2).\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f951ed",
   "metadata": {},
   "source": [
    "### 1: LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5cac4",
   "metadata": {},
   "source": [
    "#### 1.1: PUSH THE DATASET TO HUGGING-FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317477c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_hf_dataset(\n",
    "    hf_token: str,\n",
    "    data_file_path: str,\n",
    "    repo_name: str,\n",
    "    file_format: str = \"csv\",\n",
    "    split_name: str = \"test\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Uploads a local file as a Hugging Face Dataset.\n",
    "\n",
    "    Args:\n",
    "        hf_token: Your Hugging Face access token.\n",
    "        data_file_path: Path to the local data file.\n",
    "        repo_name: The target repo on HF (e.g. \"username/my-dataset\").\n",
    "        file_format: One of \"csv\", \"json\", \"tsv\", etc. Default \"csv\".\n",
    "        split_name: Name of the dataset split (e.g. \"train\", \"test\"). Default \"test\".\n",
    "    \"\"\"\n",
    "    # 1) Authenticate to HuggingFace.\n",
    "    login(token=hf_token)\n",
    "\n",
    "    # 2) Load local file.\n",
    "    data_files = { split_name: data_file_path }\n",
    "    dataset = load_dataset(file_format, data_files=data_files)\n",
    "\n",
    "    # 3) Push to Hub.\n",
    "    dataset.push_to_hub(repo_name, token=hf_token)\n",
    "    print(f\"Dataset available at https://huggingface.co/datasets/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3534962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 134.88ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset available at https://huggingface.co/datasets/Alessio-Borgi/archaic-italian-cleaned-test\n"
     ]
    }
   ],
   "source": [
    "hf_token = \"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\"\n",
    "local_path = \"/Users/alessioborgi/GitHub/AMT-AutomaticMachineTranslation/test_data/dataset_cleaned.csv\"\n",
    "repo_name  = \"Alessio-Borgi/archaic-italian-cleaned-test\"\n",
    "\n",
    "upload_to_hf_dataset(\n",
    "    hf_token=hf_token,\n",
    "    data_file_path=local_path,\n",
    "    repo_name=repo_name,\n",
    "    file_format=\"csv\",\n",
    "    split_name=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78d895",
   "metadata": {},
   "source": [
    "#### 1.2: LOADING DATASET FROM HUGGING-FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45de4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Alessio-Borgi/archaic-italian-cleaned-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a715151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['Author', 'Date', 'Region', 'Sentence'],\n",
       "        num_rows: 97\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6632166c",
   "metadata": {},
   "source": [
    "#### 1.3: EXPLORING THE TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e34f5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(dataset_name):\n",
    "    ''' Function to explore a dataset. '''\n",
    "\n",
    "    # Loading the dataset.\n",
    "    ds = load_dataset(dataset_name)\n",
    "    df = pd.DataFrame(ds[\"test\"])\n",
    "\n",
    "    # 1) Number of examples.\n",
    "    print(\"Number of examples:\", len(df))\n",
    "\n",
    "    # 2) Preview first 5 examples.\n",
    "    print(\"First 5 examples:\")\n",
    "    print(df.head(5), \"\\n\")\n",
    "\n",
    "    # 3) Sentence-length statistics.\n",
    "    df[\"length_tokens\"] = df[\"Sentence\"].apply(lambda x: len(x.split()))\n",
    "    print(\"Sentence length (tokens) stats:\")\n",
    "    print(df[\"length_tokens\"].describe(), \"\\n\")\n",
    "\n",
    "    # 4 Take out the column names.\n",
    "    print(\"Column names:\", df.columns.tolist(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c325d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 97\n",
      "First 5 examples:\n",
      "                        Author     Date Region  \\\n",
      "0              Brunetto Latini  1260-61  fior.   \n",
      "1                Bono Giamboni     1292  fior.   \n",
      "2     Valerio Massimo (red. V1     1336  fior.   \n",
      "3  Lucano volg. (ed. Marinoni)  1330/40  prat.   \n",
      "4              Brunetto Latini  1260-61  fior.   \n",
      "\n",
      "                                            Sentence  \n",
      "0  quella guerra ben fatta l' opera perché etc. E...  \n",
      "1  crudele, e di tutte le colpe pigli vendetta, c...  \n",
      "2  Non d' altra forza d' animo fue ornato Ponzio ...  \n",
      "3  Se questo piace a tutti e se 'l tempo hae biso...  \n",
      "4  Officio di questa arte pare che sia dicere app...   \n",
      "\n",
      "Sentence length (tokens) stats:\n",
      "count    97.000000\n",
      "mean     20.041237\n",
      "std       5.996384\n",
      "min       6.000000\n",
      "25%      16.000000\n",
      "50%      20.000000\n",
      "75%      24.000000\n",
      "max      31.000000\n",
      "Name: length_tokens, dtype: float64 \n",
      "\n",
      "Column names: ['Author', 'Date', 'Region', 'Sentence', 'length_tokens'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset.\n",
    "explore_dataset(dataset_name=\"Alessio-Borgi/archaic-italian-cleaned-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c38e6",
   "metadata": {},
   "source": [
    "### 2: AMT - TRANSFORMER-BASED "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ef6df",
   "metadata": {},
   "source": [
    "#### 2.1: mBART (MULTILINGUAL BART)\n",
    "\n",
    "**ARCHITECTURE & SIZE** \n",
    "This Transformer-based solution consists in 12-layer encoder + 12-layer decoder Transformer (≈610 M parameters).\n",
    "\n",
    "**DESCRIPTION**\n",
    "- **Pretraining**: It has been pretrained via Denoising auto-encoding on monolingual corpora in 50 languages (mBART-50).\n",
    "- **Multilingual MT**: It has been fine-tuned on many-to-many bitext and supports direct “it→it” by forcing Italian as both source & target.\n",
    "\n",
    "**REFERENCE INFORMATION**\n",
    "- Hugging-Face Reference page: https://huggingface.co/docs/transformers/model_doc/mbart \n",
    "- Specific Model employed: *facebook/mbart-large-50-many-to-many-mmt*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1ec3f7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMBart50Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1) Loading mBART-50 Model & Tokenizer.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/mbart-large-50-many-to-many-mmt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m mBART_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMBart50Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m      4\u001b[0m mBART_model \u001b[38;5;241m=\u001b[39m MBartForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      5\u001b[0m mBART_tokenizer\u001b[38;5;241m.\u001b[39msrc_lang \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit_IT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/GitHub/AMT-AutomaticMachineTranslation/.venv/lib/python3.9/site-packages/transformers/utils/import_utils.py:1885\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_dummy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1885\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/AMT-AutomaticMachineTranslation/.venv/lib/python3.9/site-packages/transformers/utils/import_utils.py:1871\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1868\u001b[0m         failed\u001b[38;5;241m.\u001b[39mappend(msg\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1871\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nMBart50Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading mBART-50 Model & Tokenizer.\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "mBART_tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "mBART_model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "mBART_tokenizer.src_lang = \"it_IT\"\n",
    "\n",
    "# 2) Batched translation function.\n",
    "def modernize_mbart(sentences, batch_size=8):\n",
    "    ''' Function to translate sentences using mBART. '''\n",
    "    # Instantiating the list to store translations.\n",
    "    translations = []\n",
    "\n",
    "    # Tokenize and generate translations in batches.\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        # Create a batch of sentences.\n",
    "        batch = sentences[i : i + batch_size]\n",
    "\n",
    "        # Tokenize the batch.\n",
    "        inputs = mBART_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        # Generate translations.\n",
    "        gen = mBART_model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=mBART_tokenizer.lang_code_to_id[\"it_IT\"],\n",
    "            max_length=512,\n",
    "        )\n",
    "        # Decode the generated tokens and append to translations.\n",
    "        translations.extend(mBART_tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "    return translations\n",
    "\n",
    "# 3) Run on the test split (replace \"text\" with the actual column name if different)\n",
    "arch_sentences = ds[\"Sentence\"]\n",
    "mbart_outputs = modernize_mbart(arch_sentences)\n",
    "\n",
    "# 4) Attach back to the dataset the translations.\n",
    "translated_ds_mbart = ds.add_column(\"mbart_translation\", mbart_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914d9a9",
   "metadata": {},
   "source": [
    "#### 2.2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5bbff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Translate with NLLB-200-3.3B\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "# 1) Load the test dataset\n",
    "ds = load_dataset(\"Alessio-Borgi/archaic-italian-cleaned-test\")\n",
    "\n",
    "# 2) Load NLLB-200-3.3B model & tokenizer\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer.src_lang = \"ita_Latn\"\n",
    "\n",
    "# 3) Batched translation function\n",
    "def modernize_nllb(sentences, batch_size=8):\n",
    "    translations = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i : i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        gen = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[\"ita_Latn\"],\n",
    "            max_length=512,\n",
    "        )\n",
    "        translations.extend(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "    return translations\n",
    "\n",
    "# 4) Run on your test split (replace \"text\" with the actual column name if different)\n",
    "arch_sentences = ds[\"test\"][\"text\"]\n",
    "nllb_outputs = modernize_nllb(arch_sentences)\n",
    "\n",
    "# 5) (Optional) Attach back to the dataset\n",
    "translated_ds_nllb = ds[\"test\"].add_column(\"nllb_translation\", nllb_outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
